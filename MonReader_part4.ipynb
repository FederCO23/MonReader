{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22c4196",
   "metadata": {},
   "source": [
    "## MonReader - part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956a14e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b8abb",
   "metadata": {},
   "source": [
    "### Text-to-Speech Generation from OCR Transcriptions (Audiobook-Style Rendering)\n",
    "\n",
    "**Objective.**  \n",
    "Transform the page-level OCR transcriptions produced in **MonReader – Part 3** into **spoken audio**, enabling end-to-end document accessibility and audiobook-style consumption.  \n",
    "This part evaluates modern **Text-to-Speech (TTS)** models on long-form literary text, focusing on intelligibility, prosody, multilingual robustness, and generation efficiency.\n",
    "\n",
    "We use the same two sources as previous parts:\n",
    "- *The Chamber* — John Grisham *(English)*\n",
    "- *A onda que se ergueu no mar* — Ruy Castro *(Portuguese)*\n",
    "\n",
    "All text input is sourced from the verified per-page `.txt` outputs generated by the best-performing OCR model (**Qwen2.5-VL**), ensuring a clean and stable foundation for speech synthesis.\n",
    "\n",
    "**Why this experiment.**  \n",
    "While OCR converts visual documents into text, **Text-to-Speech closes the accessibility loop**, enabling hands-free reading, audio archiving, and downstream multimodal applications helping the blind, the researchers and for everyone else in need for fully automatic, highly fast and high-quality document scanning in bulk.\n",
    "\n",
    "Long-form literary TTS presents challenges beyond short prompts: paragraph continuity, punctuation-aware prosody, proper-noun pronunciation, multilingual phonemes, and stability over extended generation windows. This experiment measures how well current TTS systems handle these constraints in a realistic, book-scale setting.\n",
    "\n",
    "**Proposed Pipeline Overview**\n",
    "\n",
    "1. **I – Data Preparation**  \n",
    "   Normalize and structure OCR text for speech synthesis: page ordering, light TTS-oriented text normalization (hyphen joins, punctuation handling), language tagging, and chunking to respect model limits.\n",
    "\n",
    "2. **J – TTS Model Benchmarking**  \n",
    "   Evaluate three leading Text-to-Speech models on representative excerpts from both books.  \n",
    "   Comparison dimensions include generation speed, pronunciation accuracy, prosody, multilingual handling (English vs. Portuguese), and long-form stability.\n",
    "\n",
    "3. **K – Full Audio Generation**  \n",
    "   Using the selected TTS model and fixed configuration, generate per-page audio files for both books.  \n",
    "   Outputs include structured audio manifests and reproducible file layouts suitable for later merging into chapter- or book-level recordings.\n",
    "\n",
    "> This part prioritizes **reproducibility and minimal intervention**: the OCR pipeline remains unchanged, and only lightweight, TTS-specific normalization is applied to the extracted text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e935cd",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b387d87",
   "metadata": {},
   "source": [
    "#### Imports and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dad98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77db430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path.cwd()\n",
    "WORK_DIR = BASE / \"work\"\n",
    "\n",
    "# OCR outputs from Part 3\n",
    "STEPH_DIR = WORK_DIR / \"stepH_qwen2p5vl_full\"\n",
    "\n",
    "# Part 4 outputs\n",
    "STEP4_DIR = WORK_DIR / \"step4_tts\"\n",
    "STEP4_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a5ee4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ec0ef",
   "metadata": {},
   "source": [
    "### Step I — Data Preparation for Text-to-Speech\n",
    "\n",
    "**Goal.**  \n",
    "Transform OCR-derived page outputs into **clean, continuous, speech-ready text** while preserving semantic content and correct reading order.\n",
    "\n",
    "This step performs **strictly minimal, TTS-oriented processing**, with no OCR correction or semantic rewriting. The focus is on structural consistency and speech robustness.\n",
    "\n",
    "**Processing stages in this step:**\n",
    "\n",
    "1) inspection of per-page OCR JSON structure\n",
    "2) explicit verification of page ordering\n",
    "3) extraction and concatenation of page-level text\n",
    "4) removal of OCR and formatting artifacts\n",
    "5) repair of hyphenated and line-wrapped words\n",
    "6) generation of canonical continuous prose\n",
    "7) sentence-aware chunking to respect TTS model limits\n",
    "\n",
    "\n",
    "No linguistic enhancement, paraphrasing, or content modification is applied. The output of this step serves as the **single source of truth** for all subsequent Text-to-Speech benchmarking and audio generation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581f814",
   "metadata": {},
   "source": [
    "#### Step I.1 - Inspection of per-page OCR JSON structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9acd7",
   "metadata": {},
   "source": [
    "Each page has one JSON like:\n",
    "\n",
    "```pgsql\n",
    "work/\n",
    "└─ stepH_qwen2p5vl_full/\n",
    "   └─ <BOOK_ID>/\n",
    "      └─ json/\n",
    "         └─ pagXX.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c1935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b2c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': 'pag12.JPEG',\n",
      " 'image_path': 'e:\\\\Devs\\\\pyEnv-1\\\\Apziva\\\\MonReader\\\\data\\\\books\\\\A_onda_que_se_ergueu_no_mar-Ruy_Castro\\\\images\\\\pag12.JPEG',\n",
      " 'latency_s': 870.2552971839905,\n",
      " 'model': 'qwen2.5vl:latest',\n",
      " 'options': {'num_predict': 1536,\n",
      "             'repeat_penalty': 1.25,\n",
      "             'stop': ['}\\n', '}\\r\\n', '}'],\n",
      "             'temperature': 0,\n",
      "             'top_p': 1},\n",
      " 'parsed': {'degenerate': False,\n",
      "            'json_objs': 0,\n",
      "            'language': 'guess',\n",
      "            'lines': ['{',\n",
      "                      '  \"language\": \"por\",',\n",
      "                      '  \"lines\": [',\n",
      "                      '    \"A trilha\",',\n",
      "                      '    \"sonora de um\",',\n",
      "                      '    \"país ideal\",',\n",
      "                      '    \"\",',\n",
      "                      '    \"Olha que coisa mais linda: as garotas de '\n",
      "                      'Ipanema-1961\",',\n",
      "                      '    \"tomavam cuba-libre, dirigiam Kharman-Ghias e '\n",
      "                      'voavam\",',\n",
      "                      '    \"pela Panair. Usavam frasqueira, vestido-tubinho, '\n",
      "                      'cílio\",',\n",
      "                      '    \"postiço, peruca, laquê. Diziam-se '\n",
      "                      'existencialistas, adoravam\",',\n",
      "                      '    \"arte abstrata e não perdiam um filme da Nouvelle '\n",
      "                      'Vague. Seus\",',\n",
      "                      '    \"points eram o Beco das Garrafas, a Cinemateca, o '\n",
      "                      'Arpoador. Iam\",',\n",
      "                      '    \"à praia com a camisa social do irmão e, sob esta, '\n",
      "                      'um biquíni que\",',\n",
      "                      '    \"de tão insolente, fazia o sangue dos rapazes '\n",
      "                      'ferver da maneira\",',\n",
      "                      '    \"mais inconveniente.\",',\n",
      "                      '    \"Tudo isso passou. A querida Panair nunca mais '\n",
      "                      'voou, a\",',\n",
      "                      '    \"Nouvelle Vague é um filme em preto e branco e '\n",
      "                      'ninguém mais\",',\n",
      "                      '    \"toma cuba-libre — quem pensaria hoje em misturar '\n",
      "                      'rum com\",',\n",
      "                      '    \"Coca-Cola? Quanto àquele biquíni, era mesmo '\n",
      "                      'insolente, embora\",',\n",
      "                      '    \"por padrões subsequentes, sua calcinha contivesse '\n",
      "                      'pano\",',\n",
      "                      '    \"para fabricar dois ou três pára-quedas. Dito '\n",
      "                      'assim, é como se, em\",',\n",
      "                      '    \"1961, o céu do Brasil ainda fosse povoado por '\n",
      "                      'pterodáctilos.\",',\n",
      "                      '    \"Mas há uma exceção. A música que aquelas garotas '\n",
      "                      'escutavam\",',\n",
      "                      '    \"na época continua a ser ouvida — um milênio depois '\n",
      "                      '—\",',\n",
      "                      '    \"como se brotasse das esferas: a Bossa Nova.\",',\n",
      "                      '    \"Acredite ou não, em números absolutos ouve-se mais '\n",
      "                      'Bossa\",',\n",
      "                      '    \"Nova hoje do que em 1961. E ela não brota das '\n",
      "                      'esferas, mas\",',\n",
      "                      '    \"é produzida ao vivo, pelos gogós, dedos e pulmões '\n",
      "                      'de artistas\",',\n",
      "                      '    \"de todas as idades, em lugares fechados ou ao ar '\n",
      "                      'livre, em quatro\",',\n",
      "                      '    \"ou cinco continentes. Ouviu-se Bossa Nova em salas '\n",
      "                      'de concerto\",',\n",
      "                      '    \"teatros, boates, bares, clubes, escolas, estádios, '\n",
      "                      'praças, praias\",',\n",
      "                      '    \"e quiosques e, ultimamente, como uma epidemia, nas '\n",
      "                      'ruas noturnas\"',\n",
      "                      '  ]'],\n",
      "            'parse_error': 'JSONDecodeError(\"Expecting \\',\\' delimiter: line '\n",
      "                           '35 column 1 (char 1785)\")',\n",
      "            'parsed_json': False},\n",
      " 'prompt': 'You are an OCR transcriber.\\n'\n",
      "           '\\n'\n",
      "           'Return ONLY one valid JSON object with keys:\\n'\n",
      "           '- \"language\": one of [\"eng\",\"por\",\"guess\"]\\n'\n",
      "           '- \"lines\": an array of strings, one per line in reading order\\n'\n",
      "           '\\n'\n",
      "           'Rules:\\n'\n",
      "           '- Do NOT repeat the JSON object.\\n'\n",
      "           '- Do NOT include any text outside the single JSON object.\\n'\n",
      "           '- Preserve line breaks and punctuation exactly as seen.\\n'\n",
      "           '- If unsure about a character, copy it as best you can (do not '\n",
      "           'explain).\\n'\n",
      "           '\\n'\n",
      "           'Transcribe the image verbatim.',\n",
      " 'raw_response': '{\\n'\n",
      "                 '  \"language\": \"por\",\\n'\n",
      "                 '  \"lines\": [\\n'\n",
      "                 '    \"A trilha\",\\n'\n",
      "                 '    \"sonora de um\",\\n'\n",
      "                 '    \"país ideal\",\\n'\n",
      "                 '    \"\",\\n'\n",
      "                 '    \"Olha que coisa mais linda: as garotas de '\n",
      "                 'Ipanema-1961\",\\n'\n",
      "                 '    \"tomavam cuba-libre, dirigiam Kharman-Ghias e voavam\",\\n'\n",
      "                 '    \"pela Panair. Usavam frasqueira, vestido-tubinho, '\n",
      "                 'cílio\",\\n'\n",
      "                 '    \"postiço, peruca, laquê. Diziam-se existencialistas, '\n",
      "                 'adoravam\",\\n'\n",
      "                 '    \"arte abstrata e não perdiam um filme da Nouvelle Vague. '\n",
      "                 'Seus\",\\n'\n",
      "                 '    \"points eram o Beco das Garrafas, a Cinemateca, o '\n",
      "                 'Arpoador. Iam\",\\n'\n",
      "                 '    \"à praia com a camisa social do irmão e, sob esta, um '\n",
      "                 'biquíni que\",\\n'\n",
      "                 '    \"de tão insolente, fazia o sangue dos rapazes ferver da '\n",
      "                 'maneira\",\\n'\n",
      "                 '    \"mais inconveniente.\",\\n'\n",
      "                 '    \"Tudo isso passou. A querida Panair nunca mais voou, '\n",
      "                 'a\",\\n'\n",
      "                 '    \"Nouvelle Vague é um filme em preto e branco e ninguém '\n",
      "                 'mais\",\\n'\n",
      "                 '    \"toma cuba-libre — quem pensaria hoje em misturar rum '\n",
      "                 'com\",\\n'\n",
      "                 '    \"Coca-Cola? Quanto àquele biquíni, era mesmo insolente, '\n",
      "                 'embora\",\\n'\n",
      "                 '    \"por padrões subsequentes, sua calcinha contivesse '\n",
      "                 'pano\",\\n'\n",
      "                 '    \"para fabricar dois ou três pára-quedas. Dito assim, é '\n",
      "                 'como se, em\",\\n'\n",
      "                 '    \"1961, o céu do Brasil ainda fosse povoado por '\n",
      "                 'pterodáctilos.\",\\n'\n",
      "                 '    \"Mas há uma exceção. A música que aquelas garotas '\n",
      "                 'escutavam\",\\n'\n",
      "                 '    \"na época continua a ser ouvida — um milênio depois —\",\\n'\n",
      "                 '    \"como se brotasse das esferas: a Bossa Nova.\",\\n'\n",
      "                 '    \"Acredite ou não, em números absolutos ouve-se mais '\n",
      "                 'Bossa\",\\n'\n",
      "                 '    \"Nova hoje do que em 1961. E ela não brota das esferas, '\n",
      "                 'mas\",\\n'\n",
      "                 '    \"é produzida ao vivo, pelos gogós, dedos e pulmões de '\n",
      "                 'artistas\",\\n'\n",
      "                 '    \"de todas as idades, em lugares fechados ou ao ar livre, '\n",
      "                 'em quatro\",\\n'\n",
      "                 '    \"ou cinco continentes. Ouviu-se Bossa Nova em salas de '\n",
      "                 'concerto\",\\n'\n",
      "                 '    \"teatros, boates, bares, clubes, escolas, estádios, '\n",
      "                 'praças, praias\",\\n'\n",
      "                 '    \"e quiosques e, ultimamente, como uma epidemia, nas ruas '\n",
      "                 'noturnas\"\\n'\n",
      "                 '  ]\\n',\n",
      " 'status': 200,\n",
      " 'timestamp_utc': '2026-01-02T19:09:44Z'}\n"
     ]
    }
   ],
   "source": [
    "# print the JSON structure of the page12 from the first book\n",
    "BOOK_ID = \"A_onda_que_se_ergueu_no_mar-Ruy_Castro\"\n",
    "PAGE = \"pag12\"\n",
    "\n",
    "json_path = Path(\"work/stepH_qwen2p5vl_full\") / BOOK_ID / \"json\" / f\"{PAGE}.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af94085",
   "metadata": {},
   "source": [
    "#### Step I.2 - Explicit verification of page ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e490e",
   "metadata": {},
   "source": [
    "Page filenames like \"pag2.JPEG\" vs \"pag10.JPEG\" will sort incorrectly with plain string sorting.\n",
    "We define an explicit \"natural sort\" that extracts the numeric page id and sorts by it, the functions were included in the `MonReader_tools.py` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dfc6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import MonReader_tools\n",
    "\n",
    "importlib.reload(MonReader_tools)\n",
    "\n",
    "from MonReader_tools import (\n",
    "    page_number_from_name, list_pages_sorted, verify_page_order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95f921bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> The_Chamber-John_Grisham (json)\n",
      "Directory: e:\\Devs\\pyEnv-1\\Apziva\\MonReader\\work\\stepH_qwen2p5vl_full\\The_Chamber-John_Grisham\\json\n",
      "Order preview:\n",
      " - pag0.json\n",
      " - pag2.json\n",
      " - pag4.json\n",
      " - pag6.json\n",
      " - pag8.json\n",
      " - pag10.json\n",
      " - pag12.json\n",
      " - pag14.json\n",
      " - pag16.json\n",
      " - pag18.json\n",
      " - pag20.json\n",
      " - pag22.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>page_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pag0.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pag2.json</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pag4.json</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pag6.json</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pag8.json</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pag10.json</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pag12.json</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pag14.json</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pag16.json</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pag18.json</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pag20.json</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pag22.json</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  page_n\n",
       "0    pag0.json       0\n",
       "1    pag2.json       2\n",
       "2    pag4.json       4\n",
       "3    pag6.json       6\n",
       "4    pag8.json       8\n",
       "5   pag10.json      10\n",
       "6   pag12.json      12\n",
       "7   pag14.json      14\n",
       "8   pag16.json      16\n",
       "9   pag18.json      18\n",
       "10  pag20.json      20\n",
       "11  pag22.json      22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> A_onda_que_se_ergueu_no_mar-Ruy_Castro (json)\n",
      "Directory: e:\\Devs\\pyEnv-1\\Apziva\\MonReader\\work\\stepH_qwen2p5vl_full\\A_onda_que_se_ergueu_no_mar-Ruy_Castro\\json\n",
      "Order preview:\n",
      " - pag12.json\n",
      " - pag16.json\n",
      " - pag18.json\n",
      " - pag20.json\n",
      " - pag22.json\n",
      " - pag24.json\n",
      " - pag26.json\n",
      " - pag28.json\n",
      " - pag32.json\n",
      " - pag36.json\n",
      " - pag40.json\n",
      " - pag44.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>page_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pag12.json</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pag16.json</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pag18.json</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pag20.json</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pag22.json</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pag24.json</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pag26.json</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pag28.json</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pag32.json</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pag36.json</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pag40.json</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pag44.json</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  page_n\n",
       "0   pag12.json      12\n",
       "1   pag16.json      16\n",
       "2   pag18.json      18\n",
       "3   pag20.json      20\n",
       "4   pag22.json      22\n",
       "5   pag24.json      24\n",
       "6   pag26.json      26\n",
       "7   pag28.json      28\n",
       "8   pag32.json      32\n",
       "9   pag36.json      36\n",
       "10  pag40.json      40\n",
       "11  pag44.json      44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run verification for both books (Step H JSON outputs)\n",
    "\n",
    "BOOKS = [\n",
    "    (\"The_Chamber-John_Grisham\", \"eng\"),\n",
    "    (\"A_onda_que_se_ergueu_no_mar-Ruy_Castro\", \"por\"),\n",
    "]\n",
    "\n",
    "dfs = {}\n",
    "for b,_ in BOOKS:\n",
    "    dfs[b] = verify_page_order(b, which=\"json\", preview=50)\n",
    "    display(dfs[b].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684541e",
   "metadata": {},
   "source": [
    "### Step I.3 - extraction and concatenation of page-level text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c89cf",
   "metadata": {},
   "source": [
    "Goal:\n",
    "- Read all per-page JSON artifacts in *correct numeric order*\n",
    "- Extract OCR text lines robustly (prefer parsed.lines when valid; otherwise parse raw_response)\n",
    "- Concatenate into a single continuous text per book (still preserving paragraph breaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1076afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(MonReader_tools)\n",
    "\n",
    "from MonReader_tools import (\n",
    "    coerce_lines, extract_first_json_obj, extract_lines_from_page_obj, join_lines_for_page, extract_book_pages, concatenate_book_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "144e7ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>page_n</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>n_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pag0.json</td>\n",
       "      <td>0</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>34</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pag2.json</td>\n",
       "      <td>2</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>68</td>\n",
       "      <td>4068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pag4.json</td>\n",
       "      <td>4</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>66</td>\n",
       "      <td>3992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pag6.json</td>\n",
       "      <td>6</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>62</td>\n",
       "      <td>3793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pag8.json</td>\n",
       "      <td>8</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>63</td>\n",
       "      <td>3888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pag10.json</td>\n",
       "      <td>10</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>69</td>\n",
       "      <td>3712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pag12.json</td>\n",
       "      <td>12</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>64</td>\n",
       "      <td>4058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pag14.json</td>\n",
       "      <td>14</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>66</td>\n",
       "      <td>3563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pag16.json</td>\n",
       "      <td>16</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>65</td>\n",
       "      <td>3513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pag18.json</td>\n",
       "      <td>18</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>67</td>\n",
       "      <td>3636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pag20.json</td>\n",
       "      <td>20</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>69</td>\n",
       "      <td>3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pag22.json</td>\n",
       "      <td>22</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>65</td>\n",
       "      <td>3551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          page  page_n language        source  n_lines  n_chars\n",
       "0    pag0.json       0    guess  raw_fallback       34     1972\n",
       "1    pag2.json       2    guess  raw_fallback       68     4068\n",
       "2    pag4.json       4    guess  raw_fallback       66     3992\n",
       "3    pag6.json       6    guess  raw_fallback       62     3793\n",
       "4    pag8.json       8    guess  raw_fallback       63     3888\n",
       "5   pag10.json      10    guess  raw_fallback       69     3712\n",
       "6   pag12.json      12    guess  raw_fallback       64     4058\n",
       "7   pag14.json      14    guess  raw_fallback       66     3563\n",
       "8   pag16.json      16    guess  raw_fallback       65     3513\n",
       "9   pag18.json      18    guess  raw_fallback       67     3636\n",
       "10  pag20.json      20    guess  raw_fallback       69     3689\n",
       "11  pag22.json      22    guess  raw_fallback       65     3551"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source\n",
      "raw_fallback    12\n",
      "Name: count, dtype: int64\n",
      "\n",
      ">>> The_Chamber-John_Grisham: concatenated_chars=43457 pages=12\n",
      "Preview (first ~1000 chars):\n",
      "{\n",
      "  \"language\": \"eng\",\n",
      "  \"lines\": [\n",
      "    \"Chapter 1 A Delicate Exercise\",\n",
      "    \"It began with a phone call on the night of April 17, 1967. Not\",\n",
      "    \"trusting his own telephone, Jeremiah Dogan drove to a pay\",\n",
      "    \"phone at a gas station to make the call. At the other end, Sam\",\n",
      "    \"Cayhall listened to the instructions he was given. When he\",\n",
      "    \"returned to bed, he told his wife nothing. She didn’t ask.\",\n",
      "    \"Two days later, Cayhall left his home town of Clanton at dusk\",\n",
      "    \"and drove to Greenville, Mississippi. There he drove slowly\",\n",
      "    \"through the center of the city, and found the offices of the\",\n",
      "    \"Jewish lawyer Marvin B. Kramer. It had been easy for the Klan*\",\n",
      "    \"to pick Kramer as their next target. He had a long history of\",\n",
      "    \"support for the civil rights movement. He led protests against\",\n",
      "    \"whites-only facilities. He accused public officials of racism.\",\n",
      "    \"He had paid for the rebuilding of a black church destroyed by\",\n",
      "    \"the Klan. He even welcomed Negroe\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>page_n</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>n_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pag12.json</td>\n",
       "      <td>12</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>34</td>\n",
       "      <td>1784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pag16.json</td>\n",
       "      <td>16</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>71</td>\n",
       "      <td>4409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pag18.json</td>\n",
       "      <td>18</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>58</td>\n",
       "      <td>3615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pag20.json</td>\n",
       "      <td>20</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pag22.json</td>\n",
       "      <td>22</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>33</td>\n",
       "      <td>1860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pag24.json</td>\n",
       "      <td>24</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>70</td>\n",
       "      <td>4331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pag26.json</td>\n",
       "      <td>26</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>41</td>\n",
       "      <td>4287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pag28.json</td>\n",
       "      <td>28</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>15</td>\n",
       "      <td>3587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pag32.json</td>\n",
       "      <td>32</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>71</td>\n",
       "      <td>4351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pag36.json</td>\n",
       "      <td>36</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>65</td>\n",
       "      <td>4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pag40.json</td>\n",
       "      <td>40</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>12</td>\n",
       "      <td>3919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pag44.json</td>\n",
       "      <td>44</td>\n",
       "      <td>guess</td>\n",
       "      <td>raw_fallback</td>\n",
       "      <td>73</td>\n",
       "      <td>4617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          page  page_n language        source  n_lines  n_chars\n",
       "0   pag12.json      12    guess  raw_fallback       34     1784\n",
       "1   pag16.json      16    guess  raw_fallback       71     4409\n",
       "2   pag18.json      18    guess  raw_fallback       58     3615\n",
       "3   pag20.json      20    guess  raw_fallback        7       68\n",
       "4   pag22.json      22    guess  raw_fallback       33     1860\n",
       "5   pag24.json      24    guess  raw_fallback       70     4331\n",
       "6   pag26.json      26    guess  raw_fallback       41     4287\n",
       "7   pag28.json      28    guess  raw_fallback       15     3587\n",
       "8   pag32.json      32    guess  raw_fallback       71     4351\n",
       "9   pag36.json      36    guess  raw_fallback       65     4183\n",
       "10  pag40.json      40    guess  raw_fallback       12     3919\n",
       "11  pag44.json      44    guess  raw_fallback       73     4617"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source\n",
      "raw_fallback    12\n",
      "Name: count, dtype: int64\n",
      "\n",
      ">>> A_onda_que_se_ergueu_no_mar-Ruy_Castro: concatenated_chars=41033 pages=12\n",
      "Preview (first ~1000 chars):\n",
      "{\n",
      "  \"language\": \"por\",\n",
      "  \"lines\": [\n",
      "    \"A trilha\",\n",
      "    \"sonora de um\",\n",
      "    \"país ideal\",\n",
      "    \"\",\n",
      "    \"Olha que coisa mais linda: as garotas de Ipanema-1961\",\n",
      "    \"tomavam cuba-libre, dirigiam Kharman-Ghias e voavam\",\n",
      "    \"pela Panair. Usavam frasqueira, vestido-tubinho, cílio\",\n",
      "    \"postiço, peruca, laquê. Diziam-se existencialistas, adoravam\",\n",
      "    \"arte abstrata e não perdiam um filme da Nouvelle Vague. Seus\",\n",
      "    \"points eram o Beco das Garrafas, a Cinemateca, o Arpoador. Iam\",\n",
      "    \"à praia com a camisa social do irmão e, sob esta, um biquíni que\",\n",
      "    \"de tão insolente, fazia o sangue dos rapazes ferver da maneira\",\n",
      "    \"mais inconveniente.\",\n",
      "    \"Tudo isso passou. A querida Panair nunca mais voou, a\",\n",
      "    \"Nouvelle Vague é um filme em preto e branco e ninguém mais\",\n",
      "    \"toma cuba-libre — quem pensaria hoje em misturar rum com\",\n",
      "    \"Coca-Cola? Quanto àquele biquíni, era mesmo insolente, embora\",\n",
      "    \"por padrões subsequentes, sua calcinha contivesse pano\",\n",
      "    \"para fabricar dois\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Step I.3 for both books\n",
    "\n",
    "book_texts = {}\n",
    "\n",
    "for book_id, _ in BOOKS:\n",
    "    df_pages = extract_book_pages(book_id)\n",
    "    display(df_pages[[\"page\",\"page_n\",\"language\",\"source\",\"n_lines\",\"n_chars\"]].head(20))\n",
    "    print(df_pages[\"source\"].value_counts(dropna=False))\n",
    "\n",
    "    full_text = concatenate_book_text(df_pages)\n",
    "    book_texts[book_id] = full_text\n",
    "\n",
    "    print(f\"\\n>>> {book_id}: concatenated_chars={len(full_text)} pages={len(df_pages)}\")\n",
    "    print(\"Preview (first ~1000 chars):\")\n",
    "    print(full_text[:1000])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de07be7",
   "metadata": {},
   "source": [
    "#### Step I.4 — Removal of OCR + formatting artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d60f0",
   "metadata": {},
   "source": [
    "Goal:\n",
    "- Remove JSON wrappers, list punctuation, and other non-content artifacts that appear in the concatenated text, while keeping the book text intact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23f10ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(MonReader_tools)\n",
    "\n",
    "from MonReader_tools import (\n",
    "    remove_json_wrapper_and_list_syntax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d571a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> The_Chamber-John_Grisham: cleaned_chars=37676\n",
      "Preview (first ~600 chars):\n",
      "Chapter 1 A Delicate Exercise\n",
      "It began with a phone call on the night of April 17, 1967. Not\n",
      "trusting his own telephone, Jeremiah Dogan drove to a pay\n",
      "phone at a gas station to make the call. At the other end, Sam\n",
      "Cayhall listened to the instructions he was given. When he\n",
      "returned to bed, he told his wife nothing. She didn’t ask.\n",
      "Two days later, Cayhall left his home town of Clanton at dusk\n",
      "and drove to Greenville, Mississippi. There he drove slowly\n",
      "through the center of the city, and found the offices of the\n",
      "Jewish lawyer Marvin B. Kramer. It had been easy for the Klan*\n",
      "to pick Kramer as thei\n",
      "\n",
      "================================================================================\n",
      "\n",
      ">>> A_onda_que_se_ergueu_no_mar-Ruy_Castro: cleaned_chars=36959\n",
      "Preview (first ~600 chars):\n",
      "A trilha\n",
      "sonora de um\n",
      "país ideal\n",
      "\n",
      "Olha que coisa mais linda: as garotas de Ipanema-1961\n",
      "tomavam cuba-libre, dirigiam Kharman-Ghias e voavam\n",
      "pela Panair. Usavam frasqueira, vestido-tubinho, cílio\n",
      "postiço, peruca, laquê. Diziam-se existencialistas, adoravam\n",
      "arte abstrata e não perdiam um filme da Nouvelle Vague. Seus\n",
      "points eram o Beco das Garrafas, a Cinemateca, o Arpoador. Iam\n",
      "à praia com a camisa social do irmão e, sob esta, um biquíni que\n",
      "de tão insolente, fazia o sangue dos rapazes ferver da maneira\n",
      "mais inconveniente.\n",
      "Tudo isso passou. A querida Panair nunca mais voou, a\n",
      "Nouvelle Vague é u\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Apply to both books\n",
    "book_texts_clean = {}\n",
    "for book_id in book_texts:\n",
    "    cleaned = remove_json_wrapper_and_list_syntax(book_texts[book_id])\n",
    "    book_texts_clean[book_id] = cleaned\n",
    "\n",
    "    print(f\"\\n>>> {book_id}: cleaned_chars={len(cleaned)}\")\n",
    "    print(\"Preview (first ~600 chars):\")\n",
    "    print(cleaned[:600])\n",
    "    print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc1aa3",
   "metadata": {},
   "source": [
    "#### Step I.5- Repair hyphenated + line-wrapped words\n",
    "\n",
    "Goal:\n",
    "- Join hyphenated line breaks: \"ex-\\nample\" -> \"example\"\n",
    "- Remove artificial line wraps inside paragraphs, while keeping paragraph breaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1361124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(MonReader_tools)\n",
    "\n",
    "from MonReader_tools import (\n",
    "    repair_hyphenation_and_wraps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "327fb5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> The_Chamber-John_Grisham: repaired_chars=37672\n",
      "Preview (first ~600 chars):\n",
      "Chapter 1 A Delicate Exercise It began with a phone call on the night of April 17, 1967. Not trusting his own telephone, Jeremiah Dogan drove to a pay phone at a gas station to make the call. At the other end, Sam Cayhall listened to the instructions he was given. When he returned to bed, he told his wife nothing. She didn’t ask. Two days later, Cayhall left his home town of Clanton at dusk and drove to Greenville, Mississippi. There he drove slowly through the center of the city, and found the offices of the Jewish lawyer Marvin B. Kramer. It had been easy for the Klan* to pick Kramer as thei\n",
      "\n",
      "================================================================================\n",
      "\n",
      ">>> A_onda_que_se_ergueu_no_mar-Ruy_Castro: repaired_chars=36765\n",
      "Preview (first ~600 chars):\n",
      "A trilha sonora de um país ideal\n",
      "\n",
      "Olha que coisa mais linda: as garotas de Ipanema-1961 tomavam cuba-libre, dirigiam Kharman-Ghias e voavam pela Panair. Usavam frasqueira, vestido-tubinho, cílio postiço, peruca, laquê. Diziam-se existencialistas, adoravam arte abstrata e não perdiam um filme da Nouvelle Vague. Seus points eram o Beco das Garrafas, a Cinemateca, o Arpoador. Iam à praia com a camisa social do irmão e, sob esta, um biquíni que de tão insolente, fazia o sangue dos rapazes ferver da maneira mais inconveniente. Tudo isso passou. A querida Panair nunca mais voou, a Nouvelle Vague é u\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Apply to both books and preview\n",
    "book_texts_repaired = {}\n",
    "for book_id, cleaned in book_texts_clean.items():\n",
    "    repaired = repair_hyphenation_and_wraps(cleaned)\n",
    "    book_texts_repaired[book_id] = repaired\n",
    "\n",
    "    print(f\"\\n>>> {book_id}: repaired_chars={len(repaired)}\")\n",
    "    print(\"Preview (first ~600 chars):\")\n",
    "    print(repaired[:600])\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd160992",
   "metadata": {},
   "source": [
    "#### Step I.6- lock a canonical, continuous prose version\n",
    "\n",
    "Goal:\n",
    "Convert the repaired text into a stable, canonical \"prose\" representation:\n",
    "- Preserve paragraph breaks as \"\\n\\n\"\n",
    "- Normalize Unicode (NFC)\n",
    "- Normalize whitespace and punctuation spacing (minimal, no rewriting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d198ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(MonReader_tools)\n",
    "\n",
    "from MonReader_tools import (\n",
    "    canonicalize_prose, count_paragraphs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8797927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> The_Chamber-John_Grisham: canonical_chars=37675 paragraphs≈1\n",
      "Preview (first ~600 chars):\n",
      "Chapter 1 A Delicate Exercise It began with a phone call on the night of April 17, 1967. Not trusting his own telephone, Jeremiah Dogan drove to a pay phone at a gas station to make the call. At the other end, Sam Cayhall listened to the instructions he was given. When he returned to bed, he told his wife nothing. She didn’t ask. Two days later, Cayhall left his home town of Clanton at dusk and drove to Greenville, Mississippi. There he drove slowly through the center of the city, and found the offices of the Jewish lawyer Marvin B. Kramer. It had been easy for the Klan* to pick Kramer as thei\n",
      "\n",
      "================================================================================\n",
      "\n",
      ">>> A_onda_que_se_ergueu_no_mar-Ruy_Castro: canonical_chars=36768 paragraphs≈2\n",
      "Preview (first ~600 chars):\n",
      "A trilha sonora de um país ideal\n",
      "\n",
      "Olha que coisa mais linda: as garotas de Ipanema-1961 tomavam cuba-libre, dirigiam Kharman-Ghias e voavam pela Panair. Usavam frasqueira, vestido-tubinho, cílio postiço, peruca, laquê. Diziam-se existencialistas, adoravam arte abstrata e não perdiam um filme da Nouvelle Vague. Seus points eram o Beco das Garrafas, a Cinemateca, o Arpoador. Iam à praia com a camisa social do irmão e, sob esta, um biquíni que de tão insolente, fazia o sangue dos rapazes ferver da maneira mais inconveniente. Tudo isso passou. A querida Panair nunca mais voou, a Nouvelle Vague é u\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "book_texts_canonical = {}\n",
    "\n",
    "for book_id, repaired_text in book_texts_repaired.items():\n",
    "    canonical = canonicalize_prose(repaired_text)\n",
    "    book_texts_canonical[book_id] = canonical\n",
    "\n",
    "    n_paras = count_paragraphs(canonical)\n",
    "    print(f\"\\n>>> {book_id}: canonical_chars={len(canonical)} paragraphs≈{n_paras}\")\n",
    "    print(\"Preview (first ~600 chars):\")\n",
    "    print(canonical[:600])\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad7227",
   "metadata": {},
   "source": [
    "#### Step I.7 — Sentence-aware chunking (TTS input)\n",
    "\n",
    "Goal: Turn canonical book text into chunks that:\n",
    "- Prefer sentence boundaries\n",
    "- Respect a max char budget (model-agnostic)\n",
    "- Preserve paragraph boundaries as soft separators\n",
    "- Are reproducible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32726d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(MonReader_tools)\n",
    "\n",
    "from MonReader_tools import (\n",
    "    chunk_text_sentence_aware, build_tts_chunks_from_canonical, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f40c10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "A_onda_que_se_ergueu_no_mar-Ruy_Castro    47\n",
      "The_Chamber-John_Grisham                  44\n",
      "Name: chunk_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A_onda_que_se_ergueu_no_mar-Ruy_Castro</th>\n",
       "      <td>47</td>\n",
       "      <td>781.319149</td>\n",
       "      <td>897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The_Chamber-John_Grisham</th>\n",
       "      <td>44</td>\n",
       "      <td>855.272727</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        count        mean  max\n",
       "book                                                          \n",
       "A_onda_que_se_ergueu_no_mar-Ruy_Castro     47  781.319149  897\n",
       "The_Chamber-John_Grisham                   44  855.272727  898"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>language</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The_Chamber-John_Grisham</td>\n",
       "      <td>0</td>\n",
       "      <td>eng</td>\n",
       "      <td>875</td>\n",
       "      <td>Chapter 1 A Delicate Exercise It began with a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The_Chamber-John_Grisham</td>\n",
       "      <td>1</td>\n",
       "      <td>eng</td>\n",
       "      <td>845</td>\n",
       "      <td>The operation had been simple to plan, as it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The_Chamber-John_Grisham</td>\n",
       "      <td>2</td>\n",
       "      <td>eng</td>\n",
       "      <td>823</td>\n",
       "      <td>1Highway 61, got in, and drove it out into ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The_Chamber-John_Grisham</td>\n",
       "      <td>3</td>\n",
       "      <td>eng</td>\n",
       "      <td>847</td>\n",
       "      <td>The two men climbed into the green Pontiac and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The_Chamber-John_Grisham</td>\n",
       "      <td>4</td>\n",
       "      <td>eng</td>\n",
       "      <td>885</td>\n",
       "      <td>\"Stay by the door and watch the alley, \" Wedge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The_Chamber-John_Grisham</td>\n",
       "      <td>5</td>\n",
       "      <td>eng</td>\n",
       "      <td>847</td>\n",
       "      <td>The train passed, and Sam took another wrong t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The_Chamber-John_Grisham</td>\n",
       "      <td>6</td>\n",
       "      <td>eng</td>\n",
       "      <td>815</td>\n",
       "      <td>\" Sam finally asked, as they turned on to High...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The_Chamber-John_Grisham</td>\n",
       "      <td>7</td>\n",
       "      <td>eng</td>\n",
       "      <td>834</td>\n",
       "      <td>Her husband Marvin helped her to the bathroom ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       book  chunk_id language  n_chars  \\\n",
       "0  The_Chamber-John_Grisham         0      eng      875   \n",
       "1  The_Chamber-John_Grisham         1      eng      845   \n",
       "2  The_Chamber-John_Grisham         2      eng      823   \n",
       "3  The_Chamber-John_Grisham         3      eng      847   \n",
       "4  The_Chamber-John_Grisham         4      eng      885   \n",
       "5  The_Chamber-John_Grisham         5      eng      847   \n",
       "6  The_Chamber-John_Grisham         6      eng      815   \n",
       "7  The_Chamber-John_Grisham         7      eng      834   \n",
       "\n",
       "                                                text  \n",
       "0  Chapter 1 A Delicate Exercise It began with a ...  \n",
       "1  The operation had been simple to plan, as it i...  \n",
       "2  1Highway 61, got in, and drove it out into ope...  \n",
       "3  The two men climbed into the green Pontiac and...  \n",
       "4  \"Stay by the door and watch the alley, \" Wedge...  \n",
       "5  The train passed, and Sam took another wrong t...  \n",
       "6  \" Sam finally asked, as they turned on to High...  \n",
       "7  Her husband Marvin helped her to the bathroom ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains JSON artifact '\", \"'? -> False\n",
      "Starts with '{' ? -> False\n"
     ]
    }
   ],
   "source": [
    "# RUN Step I.7 using canonical texts\n",
    "\n",
    "BOOK_LANG = {\n",
    "    \"The_Chamber-John_Grisham\": \"eng\",\n",
    "    \"A_onda_que_se_ergueu_no_mar-Ruy_Castro\": \"por\",\n",
    "}\n",
    "\n",
    "df_chunks = build_tts_chunks_from_canonical(\n",
    "    book_texts_canonical=book_texts_canonical,\n",
    "    book_lang=BOOK_LANG,\n",
    "    max_chars=900\n",
    ")\n",
    "\n",
    "print(df_chunks.groupby(\"book\")[\"chunk_id\"].max() + 1)     # chunks per book\n",
    "display(df_chunks.groupby(\"book\")[\"n_chars\"].agg([\"count\",\"mean\",\"max\"]))\n",
    "display(df_chunks.head(8))\n",
    "\n",
    "# Quick sanity check: should NOT contain JSON artifacts\n",
    "sample = \" \".join(df_chunks[\"text\"].head(3).tolist())\n",
    "print('Contains JSON artifact \\'\", \"\\'? ->', '\", \"' in sample)\n",
    "print(\"Starts with '{' ? ->\", sample.lstrip().startswith(\"{\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905670c2",
   "metadata": {},
   "source": [
    "Visualize the entire five chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5420d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c372a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_chunk(text: str, wrap_width: int = 100):\n",
    "    \"\"\"\n",
    "    Visualization-only formatter:\n",
    "    - splits on sentence boundaries\n",
    "    - wraps long sentences for readability\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    for s in sentences:\n",
    "        if not s:\n",
    "            continue\n",
    "        wrapped = textwrap.fill(s, width=wrap_width)\n",
    "        print(wrapped)\n",
    "        print()  # blank line between sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "868e58b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The_Chamber-John_Grisham | chunk_id=0 | n_chars=875 ===\n",
      "\n",
      "Chapter 1 A Delicate Exercise It began with a phone call on the night of April 17, 1967.\n",
      "\n",
      "Not trusting his own telephone, Jeremiah Dogan drove to a pay phone at a gas station to\n",
      "make the call.\n",
      "\n",
      "At the other end, Sam Cayhall listened to the instructions he was given.\n",
      "\n",
      "When he returned to bed, he told his wife nothing.\n",
      "\n",
      "She didn’t ask.\n",
      "\n",
      "Two days later, Cayhall left his home town of Clanton at dusk and drove to Greenville,\n",
      "Mississippi.\n",
      "\n",
      "There he drove slowly through the center of the city, and found the offices of the Jewish\n",
      "lawyer Marvin B.\n",
      "\n",
      "Kramer.\n",
      "\n",
      "It had been easy for the Klan* to pick Kramer as their next target.\n",
      "\n",
      "He had a long history of support for the civil rights movement.\n",
      "\n",
      "He led protests against whites-only facilities.\n",
      "\n",
      "He accused public officials of racism.\n",
      "\n",
      "He had paid for the rebuilding of a black church destroyed by the Klan.\n",
      "\n",
      "He even welcomed Negroes to his home.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== The_Chamber-John_Grisham | chunk_id=1 | n_chars=845 ===\n",
      "\n",
      "The operation had been simple to plan, as it involved only three people.\n",
      "\n",
      "Mississippi Klan leader Dogan provided the money, and enjoyed his role as organizer.\n",
      "\n",
      "The second man was Sam Cayhall, one of two men chosen by Dogan to do the actual dirty\n",
      "work.\n",
      "\n",
      "The Cayhall family’s connections with the Klan went back very many years, but there was\n",
      "little Klan activity in Clanton so he was considered harmless by the FBI†.\n",
      "\n",
      "He was a good choice.\n",
      "\n",
      "At eleven, Cayhall drove to Cleveland, where he looked for a green Pontiac.\n",
      "\n",
      "He found the vehicle parked at a truck stop on * The Ku Klux Klan: a secret terrorist\n",
      "organization (of ‘Klansmen’), which began in the south of the USA, targeting blacks, Jews,\n",
      "and other groups.\n",
      "\n",
      "† The FBI (Federal Bureau of Investigation): the government agency in the USA which\n",
      "investigates crimes relating to the country’s security.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== The_Chamber-John_Grisham | chunk_id=2 | n_chars=823 ===\n",
      "\n",
      "1Highway 61, got in, and drove it out into open farming country.\n",
      "\n",
      "There he stopped on a lonely road and opened the trunk.\n",
      "\n",
      "In a box covered with newspapers, he found everything he needed.\n",
      "\n",
      "Then he drove back into town and waited at an all-night café.\n",
      "\n",
      "At exactly 2 a.\n",
      "\n",
      "m., the third person in the team walked into the café and sat across from Sam Cayhall.\n",
      "\n",
      "This young man’s name was Rollie Wedge.\n",
      "\n",
      "At the age of twenty-two, Rollie was already deeply committed to the struggle for white\n",
      "power.\n",
      "\n",
      "His father was in the construction industry, and had taught his son how to use explosives.\n",
      "\n",
      "Cayhall knew little about the young man, but they had done this kind of job together\n",
      "several times now and Rollie certainly knew what he was doing.\n",
      "\n",
      "They drank coffee together for half an hour.\n",
      "\n",
      "Sam’s cup shook in his hand, but Rollie’s was steady.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== The_Chamber-John_Grisham | chunk_id=3 | n_chars=847 ===\n",
      "\n",
      "The two men climbed into the green Pontiac and, with Cayhall at the wheel, the car headed\n",
      "south on Highway 61.\n",
      "\n",
      "It was around 4 a.\n",
      "\n",
      "m.\n",
      "\n",
      "when they drove up to Kramer’s office in Greenville.\n",
      "\n",
      "The street was very quiet and dark.\n",
      "\n",
      "\"This’ll be easy,\" Rollie said softly.\n",
      "\n",
      "\"Too bad we can’t bomb his house, though.\" \"Yeah.\n",
      "\n",
      "Too bad, \" Sam agreed nervously.\n",
      "\n",
      "\"But there’s a guard at the house.\n",
      "\n",
      "And he’s got kids in there, you know.\" \"Kill them while they’re young, \" Rollie said.\n",
      "\n",
      "\"Little Jews grow up to be big ones.\" Cayhall parked the car in an alley behind Kramer’s\n",
      "office.\n",
      "\n",
      "The men quietly opened the trunk, removed the box and Rollie’s bag and moved silently\n",
      "along to the door at the back of the office.\n",
      "\n",
      "Cayhall broke open the door and in seconds they were inside.\n",
      "\n",
      "In the main hallway was a closet filled with old legal files.\n",
      "\n",
      "The perfect place for the bomb.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== The_Chamber-John_Grisham | chunk_id=4 | n_chars=885 ===\n",
      "\n",
      "\"Stay by the door and watch the alley, \" Wedge whispered, and Sam did exactly as he was\n",
      "told.\n",
      "\n",
      "He preferred not to handle the explosives himself.\n",
      "\n",
      "Rollie quickly set the box on the floor in the closet, and wired the dynamite.\n",
      "\n",
      "It was a delicate exercise, and Sam’s heart raced as he waited.\n",
      "\n",
      "He kept his back to the explosives, just in case something went wrong.\n",
      "\n",
      "They were in the office less than five minutes.\n",
      "\n",
      "In each of the bombings they had carried out before, Wedge had used a fifteen-minute fuse,\n",
      "lit with a match.\n",
      "\n",
      "The two bombers enjoyed being on the road, on the edge of the town, just as the bomb\n",
      "destroyed its target.\n",
      "\n",
      "With the car windows down they had heard and felt each of the explosions at a comfortable\n",
      "distance.\n",
      "\n",
      "But tonight was different.\n",
      "\n",
      "Sam made a wrong turn, and suddenly they were stopped at a railroad crossing as a long,\n",
      "slow train went through.\n",
      "\n",
      "Sam checked his watch.\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i, row in df_chunks.head(5).iterrows():\n",
    "    print(f\"\\n=== {row['book']} | chunk_id={row['chunk_id']} | n_chars={row['n_chars']} ===\\n\")\n",
    "    pretty_print_chunk(row[\"text\"], wrap_width=90)\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2235a6d",
   "metadata": {},
   "source": [
    "#### Wrap-up and Validation\n",
    "\n",
    "At the end of **Step I**, we have successfully transformed raw, page-level OCR outputs into a **clean, canonical, and TTS-ready text representation** for both books. The pipeline preserved reading order, semantic continuity, and narrative flow while removing OCR- and formatting-specific artifacts that would interfere with speech synthesis.\n",
    "\n",
    "The qualitative inspection of the first chunks confirms that:\n",
    "- The text is **free of JSON syntax, OCR control tokens, and structural noise**.\n",
    "- Sentence boundaries are preserved, and chunk boundaries respect natural narrative flow.\n",
    "- Dialogue, punctuation, and paragraph transitions are intact and readable.\n",
    "- Each chunk falls within a controlled character budget, making it suitable for modern TTS models without truncation or instability.\n",
    "\n",
    "Crucially, chunking is now performed **exclusively on the canonical continuous prose** derived in Step I.6, ensuring a single, well-defined source of truth for downstream processing. Visualization with line breaks is used only for human inspection and does not alter the actual TTS input.\n",
    "\n",
    "With these results, **Step I can be considered complete**:\n",
    "- Inputs to the TTS models are clean, deterministic, and reproducible.\n",
    "- The text structure aligns with best practices for long-form audiobook-style synthesis.\n",
    "- The pipeline is robust across languages (English and Portuguese) and book layouts.\n",
    "\n",
    "This provides a solid and trustworthy foundation to proceed to **Step J — Text-to-Speech Model Benchmarking**, where model quality, prosody, and long-form stability can now be evaluated without confounding data-preparation issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b809a5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59c78c",
   "metadata": {},
   "source": [
    "### Step J – TTS Model Benchmarking  \n",
    "Evaluate three leading Text-to-Speech models on representative excerpts from both books.  \n",
    "Comparison dimensions include generation speed, pronunciation accuracy, prosody, multilingual handling (English vs. Portuguese), and long-form stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328a336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonReader_env_part3",
   "language": "python",
   "name": "monreader_env_part3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
