{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22c4196",
   "metadata": {},
   "source": [
    "## MonReader - part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956a14e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b8abb",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "At this part of the project we will experience with different OCR algorightms to read pages of two different books: 'The Chamber' from John Grisham and 'A onda que se ergueu no mar' from Ruy Castro written in Portuguese.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e935cd",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b387d87",
   "metadata": {},
   "source": [
    "#### Imports and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dad98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77db430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path.cwd()\n",
    "DATA_DIR = BASE / \"data\"\n",
    "BOOK_DIR = DATA_DIR / \"books\"\n",
    "WORK_DIR = BASE / \"work\" / \"A_ingest\"\n",
    "\n",
    "ENG_BOOK_DIR = BOOK_DIR / \"The_Chamber-John_Grisham\"\n",
    "POR_BOOK_DIR = BOOK_DIR / \"A_onda_que_se_ergueu_no_mar-Ruy_Castro\"\n",
    "\n",
    "ENG_IMG_DIR = ENG_BOOK_DIR / \"images\"\n",
    "POR_IMG_DIR = POR_BOOK_DIR / \"images\"\n",
    "\n",
    "for p in [BOOK_DIR, WORK_DIR, ENG_BOOK_DIR, POR_BOOK_DIR, ENG_IMG_DIR, POR_IMG_DIR]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564e015",
   "metadata": {},
   "source": [
    "### Step 1 - Ingestion & Page conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30cf007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def natural_key(p: Path):\n",
    "    \"\"\"\n",
    "    Create a key for natural (human-like) sorting of filenames.\n",
    "    E.g., 'page2' < 'page10'.\n",
    "    \"\"\"\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', p.stem)]\n",
    "\n",
    "def collect_pages(img_dir: Path) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Collect page images (PNG/JPG/TIFF/BMP/WEBP) from a directory and return\n",
    "    a naturally sorted, de-duplicated list of paths.\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for ext in (\"*.png\",\"*.jpg\",\"*.jpeg\",\"*.tif\",\"*.tiff\",\"*.bmp\",\"*.webp\"):\n",
    "        imgs.extend(img_dir.glob(ext))\n",
    "    return sorted(set(imgs), key=natural_key)\n",
    "\n",
    "def _clip_small_angle(a_deg: float, limit: float = 15.0) -> float:\n",
    "    \"\"\"Map any angle to the nearest equivalent within [-limit, +limit] degrees.\"\"\"\n",
    "    a = ((a_deg + 90) % 180) - 90  # map to [-90, 90)\n",
    "    if a >  limit:  a -= 180\n",
    "    if a < -limit:  a += 180\n",
    "    return float(np.clip(a, -limit, limit))\n",
    "\n",
    "def estimate_skew_angle_hough(gray: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Estimate skew using Hough lines on text baselines.\n",
    "    Returns a small angle in degrees (clockwise positive means rotate CW to deskew).\n",
    "    \"\"\"\n",
    "    if gray.dtype != np.uint8:\n",
    "        gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Ignore a small border (5%) to avoid page frames dominating\n",
    "    h, w = gray.shape[:2]\n",
    "    bx = int(w * 0.05); by = int(h * 0.05)\n",
    "    roi = gray[by:h-by, bx:w-bx]\n",
    "\n",
    "    # Binarize + invert (text white)\n",
    "    _, bw = cv2.threshold(roi, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    inv = 255 - bw\n",
    "\n",
    "    # Light horizontal closing to connect text lines\n",
    "    k = max(1, h // 200)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (k*3+1, 1))\n",
    "    closed = cv2.morphologyEx(inv, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "\n",
    "    # Edges + Hough\n",
    "    edges = cv2.Canny(closed, 50, 150, apertureSize=3, L2gradient=True)\n",
    "    lines = cv2.HoughLines(edges, rho=1, theta=np.pi/180, threshold=max(80, int(min(h,w)*0.1)))\n",
    "\n",
    "    if lines is None:\n",
    "        return estimate_skew_angle_projection(gray)  # fallback\n",
    "\n",
    "    # Convert each line θ to a baseline angle around 0°\n",
    "    # HoughLines gives θ as the normal angle; baseline angle = θ - 90°\n",
    "    angles = []\n",
    "    for l in lines[:200]:  # cap to avoid noise\n",
    "        theta = l[0][1]  # radians\n",
    "        baseline_deg = np.degrees(theta) - 90.0\n",
    "        angles.append(_clip_small_angle(baseline_deg, limit=15.0))\n",
    "\n",
    "    if len(angles) == 0:\n",
    "        return estimate_skew_angle_projection(gray)\n",
    "\n",
    "    # Robust aggregate\n",
    "    median_angle = float(np.median(angles))\n",
    "    return median_angle\n",
    "\n",
    "def estimate_skew_angle_projection(gray: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Fallback: small-angle sweep maximizing horizontal projection variance.\n",
    "    Works on a downscaled image for speed.\n",
    "    \"\"\"\n",
    "    if gray.dtype != np.uint8:\n",
    "        gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Downscale for speed\n",
    "    scale = 1000 / max(gray.shape[:2])\n",
    "    if scale < 1.0:\n",
    "        small = cv2.resize(gray, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        small = gray\n",
    "\n",
    "    _, bw = cv2.threshold(small, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    inv = 255 - bw\n",
    "\n",
    "    best_angle, best_score = 0.0, -1.0\n",
    "    for a in np.linspace(-10.0, 10.0, 41):  # step 0.5°\n",
    "        # rotate around center without changing size\n",
    "        h, w = inv.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w/2, h/2), a, 1.0)\n",
    "        rot = cv2.warpAffine(inv, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "        proj = rot.sum(axis=1).astype(np.float32)\n",
    "        score = proj.var()\n",
    "        if score > best_score:\n",
    "            best_score, best_angle = score, a\n",
    "    return float(best_angle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0165a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(img: np.ndarray, angle_deg: float) -> np.ndarray:\n",
    "    \"\"\"Rotate around center with border replication.\"\"\"\n",
    "    (h, w) = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w/2, h/2), angle_deg, 1.0)\n",
    "    return cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "def detect_main_text_bbox(gray: np.ndarray) -> tuple[int,int,int,int] | None:\n",
    "    \"\"\"\n",
    "    Detect a coarse bbox for the main text region on a deskewed grayscale page.\n",
    "    Returns (x, y, w, h) or None if not found.\n",
    "    \"\"\"\n",
    "    if gray.dtype != np.uint8:\n",
    "        gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    _, bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    inv = 255 - bw\n",
    "\n",
    "    k = max(1, gray.shape[0] // 300)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (k*2+1, k*2+1))\n",
    "    proc = cv2.morphologyEx(inv, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "\n",
    "    contours, _ = cv2.findContours(proc, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "    x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
    "    return (int(x), int(y), int(w), int(h))\n",
    "\n",
    "def save_overlay(img_bgr: np.ndarray, bbox: tuple[int,int,int,int] | None) -> np.ndarray:\n",
    "    \"\"\"Draw a green rectangle over the main text region (if available).\"\"\"\n",
    "    vis = img_bgr.copy()\n",
    "    if bbox is not None:\n",
    "        x, y, w, h = bbox\n",
    "        cv2.rectangle(vis, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    return vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "166dd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main runner \n",
    "def process_image_folder(lang: str, img_dir: Path, work_dir: Path):\n",
    "    \"\"\"\n",
    "    Run Phase A1 (ingestion & page conditioning) for a language, images only.\n",
    "    Saves:\n",
    "      - page_original.png\n",
    "      - page_deskewed.png\n",
    "      - overlay_text_region.png\n",
    "    Appends per-page metadata to work/A_ingest/ingest_log.csv\n",
    "    \"\"\"\n",
    "    out_lang_dir = work_dir / lang\n",
    "    out_lang_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pages = collect_pages(img_dir)\n",
    "    log_rows = []\n",
    "\n",
    "    for page_path in tqdm(pages, desc=f\"[{lang}] ingest (images)\"):\n",
    "        pil = Image.open(page_path).convert(\"RGB\")\n",
    "        bgr = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)\n",
    "        gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Estimate skew & deskew\n",
    "        angle = estimate_skew_angle_hough(gray)\n",
    "        deskewed_bgr = rotate_image(bgr, -angle)  # rotate opposite to correct\n",
    "\n",
    "        # Text bbox on deskewed image\n",
    "        gray_deskew = cv2.cvtColor(deskewed_bgr, cv2.COLOR_BGR2GRAY)\n",
    "        bbox = detect_main_text_bbox(gray_deskew)\n",
    "        overlay = save_overlay(deskewed_bgr, bbox)\n",
    "\n",
    "        # Per-page output dir\n",
    "        page_out = out_lang_dir / page_path.stem\n",
    "        page_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save artifacts\n",
    "        cv2.imwrite(str(page_out / \"page_original.png\"), bgr)\n",
    "        cv2.imwrite(str(page_out / \"page_deskewed.png\"), deskewed_bgr)\n",
    "        cv2.imwrite(str(page_out / \"overlay_text_region.png\"), overlay)\n",
    "\n",
    "        H, W = gray.shape[:2]\n",
    "        \n",
    "        if bbox is None:\n",
    "            bx = by = bw = bh = None\n",
    "        else:\n",
    "            bx, by, bw, bh = bbox\n",
    "\n",
    "        log_rows.append({\n",
    "            \"language\": lang,\n",
    "            \"page_path\": str(page_path),\n",
    "            \"out_dir\": str(page_out),\n",
    "            \"width\": W,\n",
    "            \"height\": H,\n",
    "            \"skew_angle_deg\": float(angle),\n",
    "            \"bbox_x\": bx, \"bbox_y\": by, \"bbox_w\": bw, \"bbox_h\": bh\n",
    "        })\n",
    "\n",
    "    # Append to CSV log\n",
    "    log_csv = work_dir / \"ingest_log.csv\"\n",
    "    headers = [\"language\",\"page_path\",\"out_dir\",\"width\",\"height\",\"skew_angle_deg\",\"bbox_x\",\"bbox_y\",\"bbox_w\",\"bbox_h\"]\n",
    "    write_header = not log_csv.exists()\n",
    "    with open(log_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=headers)\n",
    "        if write_header:\n",
    "            w.writeheader()\n",
    "        w.writerows(log_rows)\n",
    "\n",
    "    print(f\"Done: {lang}. Pages: {len(pages)}. Log -> {log_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f97f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[english] ingest (images): 100%|██████████| 12/12 [00:05<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: english. Pages: 12. Log -> e:\\Devs\\pyEnv-1\\Apziva\\MonReader\\work\\A_ingest\\ingest_log.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[portuguese] ingest (images): 100%|██████████| 16/16 [00:07<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: portuguese. Pages: 16. Log -> e:\\Devs\\pyEnv-1\\Apziva\\MonReader\\work\\A_ingest\\ingest_log.csv\n",
      "Step1 (images-only) complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Step 1\n",
    "# Execute for both languages\n",
    "\n",
    "process_image_folder(\"english\", ENG_IMG_DIR, WORK_DIR)\n",
    "process_image_folder(\"portuguese\", POR_IMG_DIR, WORK_DIR)\n",
    "print(\"Step1 (images-only) complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71674dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv = WORK_DIR / \"ingest_log.csv\"\n",
    "df = pd.read_csv(log_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b12e25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>skew_angle_deg</th>\n",
       "      <th>bbox_x</th>\n",
       "      <th>bbox_y</th>\n",
       "      <th>bbox_w</th>\n",
       "      <th>bbox_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>634</td>\n",
       "      <td>0</td>\n",
       "      <td>1414</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>1.999992</td>\n",
       "      <td>881</td>\n",
       "      <td>0</td>\n",
       "      <td>1167</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>english</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>789</td>\n",
       "      <td>0</td>\n",
       "      <td>1259</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1145</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>746</td>\n",
       "      <td>0</td>\n",
       "      <td>1302</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1342</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>-2.000008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      language  width  height  skew_angle_deg  bbox_x  bbox_y  bbox_w  bbox_h\n",
       "56     english   2048    1536        0.000000       0       0    2048    1536\n",
       "57     english   2048    1536        0.000000       0       0    2048    1536\n",
       "58     english   2048    1536        0.000000       0       0    2048    1536\n",
       "59     english   2048    1536        0.000000       0       0    2048    1536\n",
       "60     english   2048    1536        0.000000       0       0    2048    1536\n",
       "61     english   2048    1536       -1.000000       0       0    2048    1536\n",
       "62     english   2048    1536        0.000000       0       0    2048    1536\n",
       "63     english   2048    1536        0.000000       0       0    2048    1536\n",
       "64     english   2048    1536        0.999992       0       0    2048    1536\n",
       "65     english   2048    1536        0.999992     634       0    1414    1536\n",
       "66     english   2048    1536        1.999992     881       0    1167    1536\n",
       "67     english   2048    1536        0.000000       0       0    2048    1536\n",
       "68  portuguese   2048    1536        0.999992     789       0    1259    1536\n",
       "69  portuguese   2048    1536        0.000000       0       0    1145    1536\n",
       "70  portuguese   2048    1536        0.999992     746       0    1302    1536\n",
       "71  portuguese   2048    1536        0.000000       0       0    2048    1536\n",
       "72  portuguese   2048    1536       15.000000       0       0    1342    1536\n",
       "73  portuguese   2048    1536        0.000000       0       0    2048    1536\n",
       "74  portuguese   2048    1536       -1.000000       0       0    2048    1536\n",
       "75  portuguese   2048    1536       -1.000000       0       0    2048    1536\n",
       "76  portuguese   2048    1536       -1.000000       0       0    2048    1536\n",
       "77  portuguese   2048    1536       -1.000000       0       0    2048    1536\n",
       "78  portuguese   2048    1536       -2.000008       0       0    2048    1536\n",
       "79  portuguese   2048    1536       -1.000000       0       0    2048    1536\n",
       "80  portuguese   2048    1536        0.000000       0       0    2048    1536\n",
       "81  portuguese   2048    1536        0.000000       0       0    2048    1536\n",
       "82  portuguese   2048    1536        0.000000       0       0    2048    1536\n",
       "83  portuguese   2048    1536        0.999992       0       0    2048    1536"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_exclude = ['page_path', 'out_dir']\n",
    "df.drop(columns=columns_to_exclude).tail(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b46754",
   "metadata": {},
   "source": [
    "#### Step 1 – Ingestion & Page Conditioning Summary\n",
    "The ingestion stage successfully collected, standardized, and deskewed all book pages, producing well-aligned images with minimal skew (mostly within ±2°) and consistent text-region detection. The resulting dataset is geometrically clean and fully logged, establishing a solid foundation for the next step binarization, where we will isolate text from background while preserving fine details and diacritics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f844f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a566b41",
   "metadata": {},
   "source": [
    "### Step 2 - Binarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f115446",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonReader_env_part2",
   "language": "python",
   "name": "monreader_env_part2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
