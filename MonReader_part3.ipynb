{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22c4196",
   "metadata": {},
   "source": [
    "## MonReader - part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956a14e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b8abb",
   "metadata": {},
   "source": [
    "### Multimodal OCR (No Pre-processing): VLM vs. Tesseract\n",
    "\n",
    "**Objective.**  \n",
    "Evaluate a **Vision–Language Model (VLM)** that performs OCR **directly from raw page images** (no deskew, no binarization, no line/word segmentation). We’ll later compare its verbatim transcription quality against **Tesseract** on the same pages.\n",
    "\n",
    "We use two sources:\n",
    "- *The Chamber* — John Grisham *(English)*\n",
    "- *A onda que se ergueu no mar* — Ruy Castro *(Portuguese)*\n",
    "\n",
    "**Why this experiment.**  \n",
    "VLMs can read document text straight from RGB photos by leveraging learned visual invariances (rotation, lighting, curvature). The aim is to measure how far a “no-preprocessing” VLM can go versus a classical pipeline, and to identify the situations where simple conditioning (e.g., deskew) still helps.\n",
    "\n",
    "**Minimal Pipeline Overview (this part).**  \n",
    "\n",
    "1. **F – VLM (raw)**: Feed the original page photo to the model with a *verbatim transcription* prompt; capture JSON output `{language, lines}` and a `.txt` view.  \n",
    "2. **G – Compare**: Compute CER/WER against gold text (and Tesseract), plus latency and error tags.\n",
    "\n",
    "> In this first section we only set up the dataset, verify image quality, and prepare folders for the VLM run, **no pre-processing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e935cd",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b387d87",
   "metadata": {},
   "source": [
    "#### Imports and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dad98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77db430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path.cwd()\n",
    "DATA_DIR = BASE / \"data\"\n",
    "BOOK_DIR = DATA_DIR / \"books\"\n",
    "WORK_DIR = BASE / \"work\"\n",
    "\n",
    "ENG_BOOK_DIR = BOOK_DIR / \"The_Chamber-John_Grisham\"\n",
    "POR_BOOK_DIR = BOOK_DIR / \"A_onda_que_se_ergueu_no_mar-Ruy_Castro\"\n",
    "\n",
    "ENG_IMG_DIR = ENG_BOOK_DIR / \"images_lr\"\n",
    "POR_IMG_DIR = POR_BOOK_DIR / \"images_lr\"\n",
    "\n",
    "for p in [BOOK_DIR, WORK_DIR, ENG_BOOK_DIR, POR_BOOK_DIR, ENG_IMG_DIR, POR_IMG_DIR]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a5ee4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ec0ef",
   "metadata": {},
   "source": [
    "### Step F — VLM OCR (GGUF local, no pre-processing)\n",
    "\n",
    "**Goal.**  \n",
    "Use a **quantized GGUF** build of *Llama 3.2-Vision Instruct* to transcribe book-page photos directly (no deskew, no binarization).  \n",
    "We start with a **single-image smoke test**, then we’ll scale to the full dataset.\n",
    "\n",
    "**Why GGUF?**  \n",
    "GGUF files are pre-quantized, self-contained weights that can run efficiently on the local GPUs through the `llama.cpp` engine (used by LM Studio and Ollama).  \n",
    "\n",
    "They trade a few points of accuracy for huge VRAM savings, perfect for a GTX 1080 Ti.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc192d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, json, time, re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b0f8a",
   "metadata": {},
   "source": [
    "### Setting up Ollama for Local Multimodal Inference\n",
    "\n",
    "Before running the OCR prompting steps, we first set up **Ollama**, a lightweight local engine for running quantized large language and vision models (GGUF format) efficiently on consumer GPUs.\n",
    "\n",
    "**Installation**\n",
    "1. Go to [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Download and install the correct version depending on your OS.\n",
    "3. Open a Terminal and verify the installation:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ollama list\n",
    "4. Pull the multimodal model:\n",
    "   ```bash\n",
    "   ollama pull llama3.2-vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4754c4",
   "metadata": {},
   "source": [
    "#### Prompt design\n",
    "\n",
    "We’ll use a **verbatim OCR prompt**. The model must output text *exactly* as it appears, with preserved line breaks and punctuation.  \n",
    "We ask for JSON to keep parsing simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c005a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL = \"llama3.2-vision\"\n",
    "IMG = Path(r\"E:\\Devs\\pyEnv-1\\Apziva\\MonReader\\data\\books\\A_onda_que_se_ergueu_no_mar-Ruy_Castro\\images_lr\\pag12.JPEG\")\n",
    "assert IMG.exists(), f\"Image not found {IMG}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5006d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b64_image(p: Path) -> str:\n",
    "    return base64.b64encode(open(p, \"rb\").read()).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec61bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 0.2\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are an OCR transcriber. Output exactly the text you see. \"\n",
    "#     \"Preserve line breaks and punctuation. \"\n",
    "#     \"Return ONLY valid JSON with keys {\\\"language\\\":\\\"eng|por|guess\\\",\\\"lines\\\":[\\\"...\\\"]}. \"\n",
    "#     \"Transcribe this image verbatim.\"\n",
    "# )\n",
    "\n",
    "# Version 0.1\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are an OCR transcriber. Return ONLY valid JSON:\\n\"\n",
    "#     '{\"language\":\"eng|por|guess\",\"lines\":[\"...\"]}\\n'\n",
    "#     \"Transcribe the image verbatim. Preserve line breaks and punctuation.\"\n",
    "# )\n",
    "\n",
    "# Version 0.2\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an OCR transcriber.\n",
    "\n",
    "Return ONLY one valid JSON object with keys:\n",
    "- \"language\": one of [\"eng\",\"por\",\"guess\"]\n",
    "- \"lines\": an array of strings, one per line in reading order\n",
    "\n",
    "Rules:\n",
    "- Do NOT repeat the JSON object.\n",
    "- Do NOT include any text outside the single JSON object.\n",
    "- Preserve line breaks and punctuation exactly as seen.\n",
    "- If unsure about a character, copy it as best you can (do not explain).\n",
    "\n",
    "Transcribe the image verbatim.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a32e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predict_values = [4096, 2048, 1024, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f858bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "resps = []\n",
    "\n",
    "for num_predict in num_predict_values:\n",
    "    t0 = time.time()\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": SYSTEM_PROMPT,\n",
    "        \"images\": [b64_image(IMG)],\n",
    "        \"format\": \"json\",\n",
    "        \"stream\": True,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"repeat_penalty\": 1.15,\n",
    "            \"num_predict\": num_predict}\n",
    "    }\n",
    "    \n",
    "    chunks = []\n",
    "    status_code = None\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        with requests.post(OLLAMA_URL, json=payload, stream=True, timeout=(10, 3600)) as r:\n",
    "            status_code = r.status_code\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            for line in r.iter_lines(decode_unicode=True):\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    # If Ollama ever emits a non-JSON line, skip or log it\n",
    "                    continue\n",
    "                \n",
    "                if \"error\" in obj and obj[\"error\"]:\n",
    "                    raise RuntimeError(f\"Ollama error: {obj['error']}\")\n",
    "                \n",
    "                chunks.append(obj.get(\"response\", \"\"))\n",
    "                \n",
    "                if obj.get(\"done\"):\n",
    "                    break\n",
    "                \n",
    "        text = \"\".join(chunks)\n",
    "        lat = time.time() - t0\n",
    "        resps.append({\"num_predict\": num_predict, \"latency_s\": lat, \"text\": text})\n",
    "        print(\"HTTP\", status_code, f\"{num_predict=} {lat:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        lat = time.time() - t0\n",
    "        print(f\"FAILED {num_predict=} after {lat:.1f}s: {e}\")\n",
    "        resps.append({\"num_predict\": num_predict, \"latency_s\": lat, \"text\": None, \"error\": str(e)})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b425e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_first_valid_json(text: str):\n",
    "    \"\"\"\n",
    "    Try to find and parse the first valid JSON object embedded in text.\n",
    "    Returns (obj, n_candidates) where:\n",
    "      - obj is a dict if found, else None\n",
    "      - n_candidates is how many {...} blocks we saw (rough proxy for repetition)\n",
    "    \"\"\"\n",
    "    # Roughly find JSON object candidates. Non-greedy to avoid swallowing everything.\n",
    "    candidates = re.findall(r\"\\{.*?\\}\", text, flags=re.DOTALL)\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            return json.loads(c), len(candidates)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None, len(candidates)\n",
    "\n",
    "def coerce_lines(js):\n",
    "    \"\"\"Normalize the 'lines' field into a list[str].\"\"\"\n",
    "    lines = js.get(\"lines\", [])\n",
    "    if isinstance(lines, str):\n",
    "        return lines.splitlines()\n",
    "    if isinstance(lines, list):\n",
    "        return [str(x) for x in lines]\n",
    "    return [str(lines)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67b9c90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_predict=4096 | parsed_json=True | json_objs≈1 | language=por | lines=28 | chars=1888\n",
      "A trilha sonora de um pais ideal\n",
      "O\n",
      "lha que coisa mais linda: as garotas de Ipanema-1961\n",
      "tomavam cuba-libre, dirigiam Kharman-Glias e voavam\n",
      "pela Panair. Usavam frasqueira, vestido-tubinho, cilio\n",
      "postico, perua, laque. Diziam-se existencialistas, adoravam\n",
      "arte abstrata e nao perdiam um filme da Nouvelle Vague.\n",
      "Seus pontos eram o Beco das Garrafas, a Cinemateca, o Arpoador.\n",
      "Iam a praia com a camisa social do irmao e, sob esta, um biquini que\n",
      "de tao insolente, fazia o sangue dos rapazes ferver da maneira\n",
      "mais incoveniente.\n",
      "Tudo isso passou. A querida Panair nunca mais voou, a\n",
      "Nouvelle Vague e um filme em preto e branco e ninguem mais\n",
      "toma cuba-libre - quem pensaria hoje em misturar rum com\n",
      "Coca-Cola? Quanto aquele biquini, era mesmo insolente, em-\n",
      "bora, por padroes subsequentes, sua calinha contivesse pano\n",
      "para fabricar dois ou tres para-ques. Dito assim, e como se, em\n",
      "1961, o ceu do Brasil ainda fosse povoado por pterodactilos.\n",
      "Mas ha uma excessao. A musica que aquelas garotas escutavam\n",
      "na epoca continua a ser ouvida - um milenio depois - como se\n",
      "brotasse das esferas: a Bossa Nova.\n",
      "Acreditou ou nao, em numeros absolutos ouve-se mais\n",
      "Bossa Nova hoje do que em 1961. Eela nao brota das esferas, mas\n",
      "e produzida ao vivo, pelos gogos, dedos e pulmoes de artistas de\n",
      "todas as idades, em lugares fechados ou ao ar livre, em quatro ou\n",
      "cinco continentes. Ouve-se Bossa Nova em salas de concerto,\n",
      "teatros, boates, bares, clubes, escolas, estadios, pracas, praiaas e\n",
      "quiosques e, ultimamente, como uma epidemia, nas ruas notur-\n",
      "================================================================================\n",
      "num_predict=2048 | parsed_json=False | json_objs≈0 | language=guess | lines=66 | chars=3599\n",
      "{\n",
      "    \"language\": \"por\",\n",
      "    \"lines\": [\n",
      "        \"A trilha sonora de um pa\\u00e1s ideal\",\n",
      "        \"O lha que coisa mais linda: as gar\\u00e9s de Ipanema-1961\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\ue9eas de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "        \"t\\u00e9m v\\u00e9s c\\u00e9s\\u00e9s de r\\u00e9s\",\n",
      "================================================================================\n",
      "num_predict=1024 | parsed_json=False | json_objs≈0 | language=guess | lines=5 | chars=1285\n",
      "{\n",
      "  \"language\": \"por\",\n",
      "  \"lines\": [\n",
      "    \"A trilha sonora de um\",\n",
      "    \"p\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas\\ua1aas  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  …  … \n",
      "================================================================================\n",
      "num_predict=512 | parsed_json=False | json_objs≈0 | language=guess | lines=4 | chars=79\n",
      "{\n",
      "\"language\": \"por\",\n",
      "\"lines\": [\n",
      "\"A trilha sonorÃ                               \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# print the responses for each 'num_predict' value\n",
    "\n",
    "for r in resps:\n",
    "    num_predict = r.get(\"num_predict\")\n",
    "    text = r.get(\"text\")\n",
    "\n",
    "    if not text:\n",
    "        print(f\"num_predict={num_predict} | EMPTY or ERROR\")\n",
    "        print(80 * \"=\")\n",
    "        continue\n",
    "\n",
    "    # Attempt 1: parse a single JSON object by trimming junk around it\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    candidate = (\n",
    "        text[start:end+1]\n",
    "        if (start != -1 and end != -1 and end > start)\n",
    "        else text\n",
    "    )\n",
    "\n",
    "    parsed_ok = False\n",
    "    lang = \"guess\"\n",
    "    lines = []\n",
    "    json_objects_found = 0\n",
    "\n",
    "    try:\n",
    "        js = json.loads(candidate)\n",
    "        lang = js.get(\"language\", \"guess\")\n",
    "        lines = coerce_lines(js)\n",
    "        parsed_ok = True\n",
    "        json_objects_found = 1  # we parsed one (assume single-object case)\n",
    "    except Exception:\n",
    "        # Attempt 2: handle repeated JSON objects / messy streams\n",
    "        js, json_objects_found = extract_first_valid_json(text)\n",
    "        if js is not None:\n",
    "            lang = js.get(\"language\", \"guess\")\n",
    "            lines = coerce_lines(js)\n",
    "            parsed_ok = True\n",
    "        else:\n",
    "            # Final fallback: plain text split\n",
    "            lines = text.splitlines()\n",
    "\n",
    "    print(\n",
    "        f\"num_predict={num_predict} | \"\n",
    "        f\"parsed_json={parsed_ok} | \"\n",
    "        f\"json_objs≈{json_objects_found} | \"\n",
    "        f\"language={lang} | \"\n",
    "        f\"lines={len(lines)} | \"\n",
    "        f\"chars={len(text)}\"\n",
    "    )\n",
    "    print(\"\\n\".join(lines[:60]))\n",
    "    print(80 * \"=\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8ac94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonReader_env_part3",
   "language": "python",
   "name": "monreader_env_part3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
