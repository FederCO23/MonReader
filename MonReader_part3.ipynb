{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22c4196",
   "metadata": {},
   "source": [
    "## MonReader - part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956a14e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b8abb",
   "metadata": {},
   "source": [
    "### Multimodal OCR (No Pre-processing): VLM vs. Tesseract\n",
    "\n",
    "**Objective.**  \n",
    "Evaluate a **Vision–Language Model (VLM)** that performs OCR **directly from raw page images** (no deskew, no binarization, no line/word segmentation). We’ll later compare its verbatim transcription quality against **Tesseract** on the same pages.\n",
    "\n",
    "We use two sources:\n",
    "- *The Chamber* — John Grisham *(English)*\n",
    "- *A onda que se ergueu no mar* — Ruy Castro *(Portuguese)*\n",
    "\n",
    "**Why this experiment.**  \n",
    "VLMs can read document text straight from RGB photos by leveraging learned visual invariances (rotation, lighting, curvature). The aim is to measure how far a “no-preprocessing” VLM can go versus a classical pipeline, and to identify the situations where simple conditioning (e.g., deskew) still helps.\n",
    "\n",
    "**Minimal Pipeline Overview (this part).**  \n",
    "\n",
    "1. **F – VLM (raw)**: Feed the original page photo to the model with a *verbatim transcription* prompt; capture JSON output `{language, lines}` and a `.txt` view.  \n",
    "2. **G – Compare**: Compute CER/WER against gold text (and Tesseract), plus latency and error tags.\n",
    "\n",
    "> In this first section we only set up the dataset, verify image quality, and prepare folders for the VLM run, **no pre-processing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e935cd",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b387d87",
   "metadata": {},
   "source": [
    "#### Imports and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dad98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77db430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path.cwd()\n",
    "DATA_DIR = BASE / \"data\"\n",
    "BOOK_DIR = DATA_DIR / \"books\"\n",
    "WORK_DIR = BASE / \"work\"\n",
    "\n",
    "ENG_BOOK_DIR = BOOK_DIR / \"The_Chamber-John_Grisham\"\n",
    "POR_BOOK_DIR = BOOK_DIR / \"A_onda_que_se_ergueu_no_mar-Ruy_Castro\"\n",
    "\n",
    "ENG_IMG_DIR = ENG_BOOK_DIR / \"images\"\n",
    "POR_IMG_DIR = POR_BOOK_DIR / \"images\"\n",
    "\n",
    "for p in [BOOK_DIR, WORK_DIR, ENG_BOOK_DIR, POR_BOOK_DIR, ENG_IMG_DIR, POR_IMG_DIR]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a5ee4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ec0ef",
   "metadata": {},
   "source": [
    "### Step F — VLM OCR (GGUF local, no pre-processing)\n",
    "\n",
    "**Goal.**  \n",
    "Use a **quantized GGUF** build of *Llama 3.2-Vision Instruct* to transcribe book-page photos directly (no deskew, no binarization).  \n",
    "We start with a **single-image smoke test**, then we’ll scale to the full dataset.\n",
    "\n",
    "**Why GGUF?**  \n",
    "GGUF files are pre-quantized, self-contained weights that can run efficiently on the local GPUs through the `llama.cpp` engine (used by LM Studio and Ollama).  \n",
    "\n",
    "They trade a few points of accuracy for huge VRAM savings, perfect for a GTX 1080 Ti.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc192d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, json, time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b0f8a",
   "metadata": {},
   "source": [
    "### Setting up Ollama for Local Multimodal Inference\n",
    "\n",
    "Before running the OCR prompting steps, we first set up **Ollama**, a lightweight local engine for running quantized large language and vision models (GGUF format) efficiently on consumer GPUs.\n",
    "\n",
    "**Installation**\n",
    "1. Go to [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Download and install the correct version depending on your OS.\n",
    "3. Open a Terminal and verify the installation:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ollama list\n",
    "4. Pull the multimodal model:\n",
    "   ```bash\n",
    "   ollama pull llama3.2-vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4754c4",
   "metadata": {},
   "source": [
    "#### Prompt design\n",
    "\n",
    "We’ll use a **verbatim OCR prompt**. The model must output text *exactly* as it appears, with preserved line breaks and punctuation.  \n",
    "We ask for JSON to keep parsing simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c005a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL = \"llama3.2-vision\"\n",
    "IMG = Path(r\"E:\\Devs\\pyEnv-1\\Apziva\\MonReader\\data\\books\\A_onda_que_se_ergueu_no_mar-Ruy_Castro\\images\\pag12.JPEG\")\n",
    "assert IMG.exists(), f\"Image not found {IMG}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5006d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b64_image(p: Path) -> str:\n",
    "    return base64.b64encode(open(p, \"rb\").read()).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec61bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are an OCR transcriber. Output exactly the text you see. \"\n",
    "    \"Preserve line breaks and punctuation. \"\n",
    "    \"Return ONLY valid JSON with keys {\\\"language\\\":\\\"eng|por|guess\\\",\\\"lines\\\":[\\\"...\\\"]}. \"\n",
    "    \"Transcribe this image verbatim.\"\n",
    ")\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are an OCR transcriber. Return ONLY valid JSON:\\n\"\n",
    "#     '{\"language\":\"eng|por|guess\",\"lines\":[\"...\"]}\\n'\n",
    "#     \"Transcribe the image verbatim. Preserve line breaks and punctuation.\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a32e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predict_values = [4096, 2048, 1024, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56c6566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\": MODEL,\n",
    "    \"prompt\": SYSTEM_PROMPT,\n",
    "    \"images\": [b64_image(IMG)],\n",
    "    \"stream\": False,\n",
    "    \"options\": { \"num_predict\": 4096 }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f858bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "resps = []\n",
    "for num_predict in num_predict_values:\n",
    "    t0 = time.time()\n",
    "    payload[\"options\"][\"num_predict\"] = num_predict\n",
    "    resp = requests.post(OLLAMA_URL, json=payload, timeout=600)\n",
    "    resps.append(resp)\n",
    "    lat = time.time() - t0\n",
    "    print(\"HTTP\", resp.status_code, f\"{num_predict=} {lat:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9c90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: eng|por|guess | Lines: 2\n",
      "A trilha sonora de um país ideal\n",
      "Iha que coisa mais linda: as garotas de Ipanema-1961 tomavam cuba-libre, dirigiam Kharman-Ghas e voavam pela Panair. Usavam frasqueira, vestido-tubinho, cilio postico, peruca, laque. Diziam-se existencialistas, adoravam arte abstrata e não perdiam um filme da Nouvelle Vague. Seus pontos eram o Beco das Garrafas, a Cinemateca, o Arpoador. Iam a praia com a camisa social do irmão e, sob esta, um biquini que, de tão insolente, fazia o sangue dos rapazes ferver da maneira mais inconveniente. Tudo isso passou. A querida Panair nunca mais voou, a Nouvelle Vague é um filme em preto e branco e ninguém mais toma cuba-libre — quem pensaria hoje em misturar rum com Coca-Cola? Quanto a quele biquini, era mesmo insolente, em-bora, por padres subsequentes, sua calça contivesse pano para fabricar dois ou três para-ques. Dito assim, é como se, em 1961, o céu do Brasil ainda fosse povoado por pterodáctilos. Mas, há uma exceção. A música que aquelas garotas escutavam na época continua a ser ouvida — um milhão de vezes — como se brotasse das esferas: a Bossa Nova. Acreditou ou não, em números absolutos, ouve-se mais Bossa Nova hoje do que em 1961. Eela não brota das esferas, mas é produzida ao vivo, pelos gogós, dedos e pulmões de artistas de todas as idades, em lugares fechados ou ao ar livre, em quatro ou cinco continentes. Ouve-se Bossa Nova em salas de concerto, teatros, boates, bares, escolas, estádios, praças, praias e quiosques e, ultimamente, como um epidemia, nas ruas noturnas.\n"
     ]
    }
   ],
   "source": [
    "# print the responses for each 'num_predict' value\n",
    "\n",
    "for resp in resps:\n",
    "    if resp.status_code != 200:\n",
    "        print(\"Error:\", resp.text[:1000])\n",
    "    else:\n",
    "        data = resp.json()\n",
    "        text = data.get(\"response\", \"\")\n",
    "\n",
    "        # Extract the JSON object if the model adds extra characters (like a trailing \".\")\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "        candidate = text[start:end+1] if (start != -1 and end != -1 and end > start) else text\n",
    "\n",
    "        try:\n",
    "            js = json.loads(candidate)\n",
    "            lang = js.get(\"language\", \"guess\")\n",
    "            lines = js.get(\"lines\", [])\n",
    "            if isinstance(lines, str):  # just in case it returns a single string\n",
    "                lines = lines.splitlines()\n",
    "        except Exception:\n",
    "            # fallback: treat as plain text\n",
    "            lang = \"guess\"\n",
    "            lines = text.splitlines()\n",
    "\n",
    "        print(f\"Detected language: {lang} | Lines: {len(lines)}\")\n",
    "        print(\"\\n\".join(lines[:60]))\n",
    "        print(80*\"=\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8ac94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonReader_env_part3",
   "language": "python",
   "name": "monreader_env_part3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
