{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22c4196",
   "metadata": {},
   "source": [
    "## MonReader - part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956a14e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b8abb",
   "metadata": {},
   "source": [
    "### Multimodal OCR (No Pre-processing): VLM vs. Tesseract\n",
    "\n",
    "**Objective.**  \n",
    "Evaluate a **Vision–Language Model (VLM)** that performs OCR **directly from raw page images** (no deskew, no binarization, no line/word segmentation). We’ll later compare its verbatim transcription quality against **Tesseract** on the same pages.\n",
    "\n",
    "We use two sources:\n",
    "- *The Chamber* — John Grisham *(English)*\n",
    "- *A onda que se ergueu no mar* — Ruy Castro *(Portuguese)*\n",
    "\n",
    "**Why this experiment.**  \n",
    "VLMs can read document text straight from RGB photos by leveraging learned visual invariances (rotation, lighting, curvature). The aim is to measure how far a “no-preprocessing” VLM can go versus a classical pipeline, and to identify the situations where simple conditioning (e.g., deskew) still helps.\n",
    "\n",
    "**Minimal Pipeline Overview (this part).**  \n",
    "\n",
    "1. **F – VLM (raw)**: Feed the original page photo to the model with a *verbatim transcription* prompt; capture JSON output `{language, lines}` and a `.txt` view.  \n",
    "2. **G – Compare**: Compute CER/WER against gold text (and Tesseract), plus latency and error tags.\n",
    "3. **H – Full Extraction**: Run the best-performing model across **all pages** of **both books**, saving per-page outputs + a run manifest for later analysis.\n",
    "\n",
    "> In this first section we only set up the dataset, verify image quality, and prepare folders for the VLM run, **no pre-processing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e935cd",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b387d87",
   "metadata": {},
   "source": [
    "#### Imports and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dad98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77db430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path.cwd()\n",
    "DATA_DIR = BASE / \"data\"\n",
    "BOOK_DIR = DATA_DIR / \"books\"\n",
    "WORK_DIR = BASE / \"work\"\n",
    "\n",
    "ENG_BOOK_DIR = BOOK_DIR / \"The_Chamber-John_Grisham\"\n",
    "POR_BOOK_DIR = BOOK_DIR / \"A_onda_que_se_ergueu_no_mar-Ruy_Castro\"\n",
    "\n",
    "ENG_IMG_DIR = ENG_BOOK_DIR / \"images\"\n",
    "POR_IMG_DIR = POR_BOOK_DIR / \"images\"\n",
    "\n",
    "for p in [BOOK_DIR, WORK_DIR, ENG_BOOK_DIR, POR_BOOK_DIR, ENG_IMG_DIR, POR_IMG_DIR]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a5ee4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ec0ef",
   "metadata": {},
   "source": [
    "### Step F — VLM OCR (GGUF local, no pre-processing)\n",
    "\n",
    "**Goal.**  \n",
    "Use a **quantized GGUF** build of *Llama 3.2-Vision Instruct* to transcribe book-page photos directly (no deskew, no binarization).  \n",
    "We start with a **single-image smoke test**, then we’ll scale to the full dataset.\n",
    "\n",
    "**Why GGUF?**  \n",
    "GGUF files are pre-quantized, self-contained weights that can run efficiently on the local GPUs through the `llama.cpp` engine (used by LM Studio and Ollama).  \n",
    "\n",
    "They trade a few points of accuracy for huge VRAM savings, perfect for a GTX 1080 Ti.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc192d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, json, time, re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b0f8a",
   "metadata": {},
   "source": [
    "### Setting up Ollama for Local Multimodal Inference\n",
    "\n",
    "Before running the OCR prompting steps, we first set up **Ollama**, a lightweight local engine for running quantized large language and vision models (GGUF format) efficiently on consumer GPUs.\n",
    "\n",
    "**Installation**\n",
    "1. Go to [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Download and install the correct version depending on your OS.\n",
    "3. Open a Terminal and verify the installation:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ollama list\n",
    "4. Pull the multimodal model:\n",
    "   ```bash\n",
    "   ollama pull llama3.2-vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4754c4",
   "metadata": {},
   "source": [
    "#### Prompt design\n",
    "\n",
    "We’ll use a **verbatim OCR prompt**. The model must output text *exactly* as it appears, with preserved line breaks and punctuation.  \n",
    "We ask for JSON to keep parsing simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c005a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL = \"llama3.2-vision\"\n",
    "IMG = Path(r\"E:\\Devs\\pyEnv-1\\Apziva\\MonReader\\data\\books\\A_onda_que_se_ergueu_no_mar-Ruy_Castro\\images\\pag12.JPEG\")\n",
    "assert IMG.exists(), f\"Image not found {IMG}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5006d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b64_image(p: Path) -> str:\n",
    "    return base64.b64encode(open(p, \"rb\").read()).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec61bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 0.2\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are an OCR transcriber. Output exactly the text you see. \"\n",
    "#     \"Preserve line breaks and punctuation. \"\n",
    "#     \"Return ONLY valid JSON with keys {\\\"language\\\":\\\"eng|por|guess\\\",\\\"lines\\\":[\\\"...\\\"]}. \"\n",
    "#     \"Transcribe this image verbatim.\"\n",
    "# )\n",
    "\n",
    "# Version 0.1\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are an OCR transcriber. Return ONLY valid JSON:\\n\"\n",
    "#     '{\"language\":\"eng|por|guess\",\"lines\":[\"...\"]}\\n'\n",
    "#     \"Transcribe the image verbatim. Preserve line breaks and punctuation.\"\n",
    "# )\n",
    "\n",
    "# Version 0.2\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an OCR transcriber.\n",
    "\n",
    "Return ONLY one valid JSON object with keys:\n",
    "- \"language\": one of [\"eng\",\"por\",\"guess\"]\n",
    "- \"lines\": an array of strings, one per line in reading order\n",
    "\n",
    "Rules:\n",
    "- Do NOT repeat the JSON object.\n",
    "- Do NOT include any text outside the single JSON object.\n",
    "- Preserve line breaks and punctuation exactly as seen.\n",
    "- If unsure about a character, copy it as best you can (do not explain).\n",
    "\n",
    "Transcribe the image verbatim.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a32e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predict_values = [1536, 2048, 3072]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f858bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 200 num_predict=1536 659.7s\n",
      "HTTP 200 num_predict=2048 825.2s\n",
      "HTTP 200 num_predict=3072 517.4s\n"
     ]
    }
   ],
   "source": [
    "resps = []\n",
    "\n",
    "for num_predict in num_predict_values:\n",
    "    t0 = time.time()\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": SYSTEM_PROMPT,\n",
    "        \"images\": [b64_image(IMG)],\n",
    "        \"format\": \"json\",\n",
    "        \"stream\": True,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"repeat_penalty\": 1.2,\n",
    "            \"num_predict\": num_predict,\n",
    "            \"stop\": [\"\\n}\\n\", \"\\n}\\r\\n\", \"\\n}\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    chunks = []\n",
    "    status_code = None\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        with requests.post(OLLAMA_URL, json=payload, stream=True, timeout=(10, 3600)) as r:\n",
    "            status_code = r.status_code\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            for line in r.iter_lines(decode_unicode=True):\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    # If Ollama ever emits a non-JSON line, skip or log it\n",
    "                    continue\n",
    "                \n",
    "                if \"error\" in obj and obj[\"error\"]:\n",
    "                    raise RuntimeError(f\"Ollama error: {obj['error']}\")\n",
    "                \n",
    "                chunks.append(obj.get(\"response\", \"\"))\n",
    "                \n",
    "                if obj.get(\"done\"):\n",
    "                    break\n",
    "                \n",
    "        text = \"\".join(chunks)\n",
    "        lat = time.time() - t0\n",
    "        resps.append({\"num_predict\": num_predict, \"latency_s\": lat, \"text\": text})\n",
    "        print(\"HTTP\", status_code, f\"{num_predict=} {lat:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        lat = time.time() - t0\n",
    "        print(f\"FAILED {num_predict=} after {lat:.1f}s: {e}\")\n",
    "        resps.append({\"num_predict\": num_predict, \"latency_s\": lat, \"text\": None, \"error\": str(e)})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4761ee",
   "metadata": {},
   "source": [
    "#### *Note*: effect of 2× Image Downscaling (other experiment, not treated in this notebook)\n",
    "\n",
    "A brief experiment was conducted using page images downscaled by a factor of 2 to evaluate whether reduced resolution would improve efficiency or stability in multimodal OCR.\n",
    "\n",
    "In practice, aggressive downscaling proved **counterproductive** for this task. While smaller images reduce pixel count, they also degrade fine visual cues critical for text recognition (thin strokes, diacritics, punctuation, hyphenation). Once these cues are weakened, the vision–language model becomes less stable and more prone to decoding artifacts and repetition.\n",
    "\n",
    "These were the registered times for different `num_predict` values (number of generated tokens):\n",
    "- HTTP 200 num_predict=4096 1036.6s\n",
    "- HTTP 200 num_predict=2048 1261.1s\n",
    "- HTTP 200 num_predict=1024 649.5s\n",
    "- HTTP 200 num_predict=512 321.7s\n",
    "\n",
    "This observation highlights an important property of VLM-based OCR: **image resolution acts as a form of implicit conditioning**. Excessive downscaling can remove information needed for reliable alignment between visual tokens and text generation, even if the model nominally supports invariance to scale.\n",
    "\n",
    "Based on this experience, subsequent experiments use **full-resolution page images**, prioritizing transcription stability and fidelity over marginal efficiency gains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b425e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_first_valid_json(text: str):\n",
    "    \"\"\"\n",
    "    Try to find and parse the first valid JSON object embedded in text.\n",
    "    Returns (obj, n_candidates) where:\n",
    "      - obj is a dict if found, else None\n",
    "      - n_candidates is how many {...} blocks we saw (rough proxy for repetition)\n",
    "    \"\"\"\n",
    "    # Roughly find JSON object candidates. Non-greedy to avoid swallowing everything.\n",
    "    candidates = re.findall(r\"\\{.*?\\}\", text, flags=re.DOTALL)\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            return json.loads(c), len(candidates)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None, len(candidates)\n",
    "\n",
    "def coerce_lines(js):\n",
    "    \"\"\"Normalize the 'lines' field into a list[str].\"\"\"\n",
    "    lines = js.get(\"lines\", [])\n",
    "    if isinstance(lines, str):\n",
    "        return lines.splitlines()\n",
    "    if isinstance(lines, list):\n",
    "        return [str(x) for x in lines]\n",
    "    return [str(lines)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67b9c90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_predict=1536 | parsed_json=False | json_objs≈0 | language=guess | lines=32 | chars=1886\n",
      "{\n",
      "    \"language\": \"por\",\n",
      "    \"lines\": [\n",
      "        \"A trilha sonora de um pais ideal\",\n",
      "        \"O\",\n",
      "        \"lha que coisa mais linda: as garotas de Ipanema-1961\",\n",
      "        \"tomavam cuba-libre, dirigiam Kharman-Glias e voavam\",\n",
      "        \"pela Panair. Usavam frasqueira, vestido-tubinho, cilio\",\n",
      "        \"postico, perua, laque. Diziam-se existencialistas, adoravam\",\n",
      "        \"arte abstrata e nao perdiam um filme da Nouvelle Vague.\",\n",
      "        \"Seus pontos eram o Beco das Garrafas, a Cinemateca, o Arpoador.\",\n",
      "        \"Iam a praia com a camisa social do irmao e, sob esta, um biquini que\",\n",
      "        \"de tao insolente, fazia o sangue dos rapazes ferver da maneira\",\n",
      "        \"mais incoveniente.\",\n",
      "        \"Tudo isso passou. A querida Panair nunca mais voou, a\",\n",
      "        \"Nouvelle Vague e um filme em preto e branco e ninguem mais\",\n",
      "        \"toma cuba-libre - quem pensaria hoje em misturar rum com\",\n",
      "        \"Coca-Cola? Quanto aquele biquini, era mesmo insolente, em-\",\n",
      "        \"bora, por padroes subsequentes, sua calinha contivesse pano\",\n",
      "        \"para fabricar dois ou tres para-ques. Dito assim, e como se, em\",\n",
      "        \"1961, o ceu do Brasil ainda fosse povoado por pterodactilos.\",\n",
      "        \"Mas ha uma excessao. A musica que aquelas garotas escutavam\",\n",
      "        \"na epoca continua a ser ouvida - um milenio depois - como se\",\n",
      "        \"brotasse das esferas: a Bossa Nova.\",\n",
      "        \"Acreditou ou nao, em numeros absolutos ouve-se mais\",\n",
      "        \"Bossa Nova hoje do que em 1961. Eela nao brota das esferas, mas\",\n",
      "        \"e produzida ao vivo, pelos gogos, dedos e pulmoes de artistas de\",\n",
      "        \"todas as idades, em lugares fechados ou ao ar livre, em quatro ou\",\n",
      "        \"cinco continentes. Ouve-se Bossa Nova em salas de concerto,\",\n",
      "        \"teatros, boates, bares, clubes, escolas, estadios, pracas, praiaas e\",\n",
      "        \"quiosques e, ultimamente, como uma epidemia, nas ruas notur-\"\n",
      "    ]\n",
      "================================================================================\n",
      "num_predict=2048 | parsed_json=False | json_objs≈0 | language=guess | lines=19 | chars=2155\n",
      "{\n",
      "    \"language\": \"por\",\n",
      "    \"lines\": [\n",
      "        \"A trilha sonora de um pa\\u00ed s\\u00ad\\u00e9l\",\n",
      "        \"O lha que coisa mais linda: as gar\\u00f9as de Ipanema-1961\",\n",
      "        \"t\\u00a9mavam c\\u00a9a-l\\u00a9bre, dirigiam Kharman-Ghias e voavam\",\n",
      "        \"pela Panair. Usavam fr\\u00a9squeira, vestido-t\\u00a9bio, c\\u00a9lio\",\n",
      "        \"posti\\u00a9o, per\\u00a9ca, la\\u00a9\\u00e9. Diziam-se existencialistas, adoravam\",\n",
      "        \"arte abstrata e n\\u00a9o perdi\\u00aam um filme da Nouvelle Vague. Seus\",\n",
      "        \"pontos eram o B\\u00e9co das Gar\\u00f9as, a Cinemateca, o Arp\\u00a9dor. I\",\n",
      "        \"a pr\\ua1acoma c\\u00a9a m\\ua1aacute; s\\ua1aacute; linda: as gar\\u00f9as de Ipanema-1961\",\n",
      "        \"t\\u00a9mavam c\\u00a9a-l\\u00a9bre, dirigiam Kharman-Ghias e voavam\",\n",
      "        \"pela Panair. Usavam fr\\u00a9squeira, vestido-t\\u00a9bio, c\\u00a9lio\",\n",
      "        \"posti\\u00a9o, per\\u00a9ca, la\\u00a9\\u00e9. Diziam-se existencialistas, adoravam\",\n",
      "        \"arte abstrata e n\\u00a9o perdi\\u00aam um filme da Nouv\\ua1a1e Vague. Seus\",\n",
      "        \"pontos eram o B\\u00e9co das Gar\\u00f9as, a Cinemateca, o Arp\\ua1a1dor. I\",\n",
      "        \"a pr\\ua1acoma c\\u00a1a m\\ua1aacute; s\\ua1aacute; linda: as gar\\u00f9as de Ipanema-1961\",\n",
      "        \"t\\ua1a1mavam c\\ua1a1a-l\\ua1a1bire, diri\\ua1a1giam Kharman-Ghias e v\\ua1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a…\"\n",
      "    ]\n",
      "================================================================================\n",
      "num_predict=3072 | parsed_json=False | json_objs≈0 | language=guess | lines=6 | chars=1001\n",
      "{\n",
      "\"language\": \"por\",\n",
      "\"lines\": [\n",
      "\"A trilha sonora de um pa\\u00a1s ideal\",\n",
      "\"O lha que coisa m\\ua1aacute; linda: as g\\ua1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a1a…\"\n",
      "]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# print the responses for each 'num_predict' value\n",
    "\n",
    "for r in resps:\n",
    "    num_predict = r.get(\"num_predict\")\n",
    "    text = r.get(\"text\")\n",
    "\n",
    "    if not text:\n",
    "        print(f\"num_predict={num_predict} | EMPTY or ERROR\")\n",
    "        print(80 * \"=\")\n",
    "        continue\n",
    "\n",
    "    # Attempt 1: parse a single JSON object by trimming junk around it\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    candidate = (\n",
    "        text[start:end+1]\n",
    "        if (start != -1 and end != -1 and end > start)\n",
    "        else text\n",
    "    )\n",
    "\n",
    "    parsed_ok = False\n",
    "    lang = \"guess\"\n",
    "    lines = []\n",
    "    json_objects_found = 0\n",
    "\n",
    "    try:\n",
    "        js = json.loads(candidate)\n",
    "        lang = js.get(\"language\", \"guess\")\n",
    "        lines = coerce_lines(js)\n",
    "        parsed_ok = True\n",
    "        json_objects_found = 1  # we parsed one (assume single-object case)\n",
    "    except Exception:\n",
    "        # Attempt 2: handle repeated JSON objects / messy streams\n",
    "        js, json_objects_found = extract_first_valid_json(text)\n",
    "        if js is not None:\n",
    "            lang = js.get(\"language\", \"guess\")\n",
    "            lines = coerce_lines(js)\n",
    "            parsed_ok = True\n",
    "        else:\n",
    "            # Final fallback: plain text split\n",
    "            lines = text.splitlines()\n",
    "\n",
    "    print(\n",
    "        f\"num_predict={num_predict} | \"\n",
    "        f\"parsed_json={parsed_ok} | \"\n",
    "        f\"json_objs≈{json_objects_found} | \"\n",
    "        f\"language={lang} | \"\n",
    "        f\"lines={len(lines)} | \"\n",
    "        f\"chars={len(text)}\"\n",
    "    )\n",
    "    print(\"\\n\".join(lines[:60]))\n",
    "    print(80 * \"=\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a45c5e",
   "metadata": {},
   "source": [
    "### Commentary: Current baseline run (full-resolution, tuned decoding)\n",
    "\n",
    "With the current prompt + decoding controls (streaming, `temperature=0`, increased `repeat_penalty`, and explicit stop tokens), the **first run (`num_predict=1536`) produced the most usable transcription** among the three caps tested. It yielded the most coherent line-by-line OCR with fewer obvious decoding artifacts.\n",
    "\n",
    "In contrast, the later runs at higher `num_predict` values showed degraded stability (character corruption, repetition, and token “runaway” patterns). This reinforces an important practical point for VLM-based OCR: **larger generation budgets do not guarantee better transcription quality**, and can amplify failure modes when the model drifts into repetition.\n",
    "\n",
    "Given these observations, `num_predict=1536` is treated as the **current baseline** for subsequent comparisons, and further tuning is deferred while we move on to cross-model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8ac94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d63c26",
   "metadata": {},
   "source": [
    "### Step F(continuation) — compare results from three popular multimodal models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8c313",
   "metadata": {},
   "source": [
    "This section summarizes the core characteristics of the three Vision–Language Models (VLMs) evaluated in this notebook:\n",
    "\n",
    "- **LLaMA 3.2 Vision Instruct**\n",
    "- **LLaVA**\n",
    "- **Qwen 2.5-VL**\n",
    "\n",
    "All models are executed locally via **Ollama** using quantized GGUF weights and evaluated under the same OCR-oriented prompting and decoding constraints. The goal is not to benchmark general vision reasoning, but to assess **verbatim transcription stability, long-form generation behavior, and robustness to document layout**.\n",
    "\n",
    "---\n",
    "\n",
    "### Vision–Language Model Comparison (Spec Sheet)\n",
    "\n",
    "| Feature | **LLaMA 3.2 Vision** | **LLaVA** | **Qwen 2.5-VL** |\n",
    "|------|---------------------|----------|----------------|\n",
    "| **Origin** | Meta AI | UW–Madison + Microsoft | Alibaba Cloud |\n",
    "| **Initial Release** | 2024 | 2023 | 2024 |\n",
    "| **Primary Design Goal** | Instruction-following multimodal assistant | General-purpose VLM | Strong multimodal + multilingual understanding |\n",
    "| **Language Backbone** | LLaMA 3.2 | LLaMA-family (7B) | Qwen 2.5 |\n",
    "| **Approx. Params (LLM)** | ~8B | ~7B | ~7B |\n",
    "| **Vision Encoder** | Proprietary ViT (Meta) | CLIP ViT | Qwen ViT-based encoder |\n",
    "| **Fusion Strategy** | Visual tokens injected into text context | Visual tokens projected into LLM space | Unified multimodal token space |\n",
    "| **Context Window (text)** | ~8k tokens | ~4k tokens | ~8k tokens |\n",
    "| **Multilingual Support** | Moderate | Limited | **Strong (incl. Portuguese)** |\n",
    "| **OCR Orientation** | Medium | Low–Medium | **High** |\n",
    "| **Verbatim Fidelity** | Good (early tokens) | Moderate | **Strong** |\n",
    "| **Long-Form Stability** | Degrades with length | Prone to repetition | **Most stable** |\n",
    "| **JSON / Structured Output** | Sometimes brittle | Brittle at long lengths | **Most consistent** |\n",
    "| **Typical Failure Modes** | Late repetition, truncation | Semantic drift, format repetition | Over-generation if unconstrained |\n",
    "| **Disk Size (Ollama)** | ~7.8 GB | ~4.7 GB | ~6.0 GB |\n",
    "| **Role in This Study** | Primary baseline | Legacy VLM baseline | OCR-oriented challenger |\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Interpretation for OCR Experiments\n",
    "\n",
    "- **LLaMA 3.2 Vision** performs well in early decoding but tends to degrade as generation length increases.\n",
    "- **LLaVA** is optimized for general visual dialogue and reasoning rather than strict verbatim transcription.\n",
    "- **Qwen 2.5-VL** offers the best balance of multilingual support, decoding stability, and OCR-aligned behavior, making it particularly suitable for document-level transcription tasks.\n",
    "\n",
    "Subsequent sections evaluate each model individually using identical prompts, decoding parameters, and page images to enable direct comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3da407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d038e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the complete list of Models that we will use\n",
    "MODELS = [\n",
    "    {\"name\": \"llama3.2-vision:latest\", \"label\": \"Llama-3.2-Vision\"},\n",
    "    {\"name\": \"llava:latest\",           \"label\": \"LLaVA\"},\n",
    "    {\"name\": \"qwen2.5vl:latest\",          \"label\": \"Qwen2.5-VL (7B)\"},\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb21538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# - We will keep the same SYSTEM PROMPT\n",
    "# - We will adapt the payload base options keeping the num_predict set as 1536 (best options after simple benchmark)\n",
    "\n",
    "\n",
    "# Baseline decoding options\n",
    "BASE_OPTIONS = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"repeat_penalty\": 1.25,\n",
    "    \"num_predict\": 1536,\n",
    "    \"stop\": [\"}\\n\", \"}\\r\\n\", \"}\"],\n",
    "}\n",
    "\n",
    "# Define a basic model runner that can switch between models\n",
    "def run_ollama_ocr(model_name: str, img_path: Path, prompt: str, options: dict):\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": [b64_image(img_path)],\n",
    "        \"stream\": True,\n",
    "        \"format\": \"json\",\n",
    "        \"options\": options,\n",
    "    }\n",
    "\n",
    "    t0 = time.time()\n",
    "    chunks = []\n",
    "    status_code = None\n",
    "\n",
    "    with requests.post(OLLAMA_URL, json=payload, stream=True, timeout=(10, 3600)) as r:\n",
    "        status_code = r.status_code\n",
    "        r.raise_for_status()\n",
    "\n",
    "        for line in r.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            if obj.get(\"error\"):\n",
    "                raise RuntimeError(f\"Ollama error: {obj['error']}\")\n",
    "\n",
    "            chunks.append(obj.get(\"response\", \"\"))\n",
    "            if obj.get(\"done\"):\n",
    "                break\n",
    "\n",
    "    text = \"\".join(chunks)\n",
    "    latency_s = time.time() - t0\n",
    "    return {\"model\": model_name, \"status\": status_code, \"latency_s\": latency_s, \"text\": text}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb286802",
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpers\n",
    "def extract_first_valid_json(text: str):\n",
    "    candidates = re.findall(r\"\\{.*?\\}\", text, flags=re.DOTALL)\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            return json.loads(c), len(candidates)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None, len(candidates)\n",
    "\n",
    "def coerce_lines(js):\n",
    "    lines = js.get(\"lines\", [])\n",
    "    if isinstance(lines, str):\n",
    "        return lines.splitlines()\n",
    "    if isinstance(lines, list):\n",
    "        return [str(x) for x in lines]\n",
    "    return [str(lines)]\n",
    "\n",
    "def parse_ocr_text(text: str):\n",
    "    if not text:\n",
    "        return {\"parsed_json\": False, \"language\": \"guess\", \"lines\": [], \"json_objs\": 0, \"parse_error\": \"empty\"}\n",
    "\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    candidate = text[start:end+1] if (start != -1 and end != -1 and end > start) else text\n",
    "\n",
    "    try:\n",
    "        js = json.loads(candidate)\n",
    "        return {\n",
    "            \"parsed_json\": True,\n",
    "            \"language\": js.get(\"language\", \"guess\"),\n",
    "            \"lines\": coerce_lines(js),\n",
    "            \"json_objs\": 1,\n",
    "            \"parse_error\": None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        js, n = extract_first_valid_json(text)\n",
    "        if js is not None:\n",
    "            return {\n",
    "                \"parsed_json\": True,\n",
    "                \"language\": js.get(\"language\", \"guess\"),\n",
    "                \"lines\": coerce_lines(js),\n",
    "                \"json_objs\": n,\n",
    "                \"parse_error\": None\n",
    "            }\n",
    "        return {\n",
    "            \"parsed_json\": False,\n",
    "            \"language\": \"guess\",\n",
    "            \"lines\": text.splitlines(),\n",
    "            \"json_objs\": n,\n",
    "            \"parse_error\": repr(e)\n",
    "        }\n",
    "\n",
    "def looks_degenerate(text: str) -> bool:\n",
    "    # quick heuristic flags: unicode junk / obvious repetition artifacts\n",
    "    if not text:\n",
    "        return True\n",
    "    if \"\\\\ud\" in text or \"\\\\ua1\" in text:\n",
    "        return True\n",
    "    if \"a1a1a1\" in text:\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abf9472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK  - Llama-3.2-Vision  (630.7s)\n",
      "OK  - LLaVA  (89.6s)\n",
      "OK  - Qwen2.5-VL (7B)  (910.7s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>model</th>\n",
       "      <th>status</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>parsed_json</th>\n",
       "      <th>language</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>json_objs</th>\n",
       "      <th>degenerate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLaVA</td>\n",
       "      <td>llava:latest</td>\n",
       "      <td>200</td>\n",
       "      <td>89.551258</td>\n",
       "      <td>False</td>\n",
       "      <td>guess</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-3.2-Vision</td>\n",
       "      <td>llama3.2-vision:latest</td>\n",
       "      <td>200</td>\n",
       "      <td>630.708090</td>\n",
       "      <td>False</td>\n",
       "      <td>guess</td>\n",
       "      <td>32</td>\n",
       "      <td>1887</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2.5-VL (7B)</td>\n",
       "      <td>qwen2.5vl:latest</td>\n",
       "      <td>200</td>\n",
       "      <td>910.669698</td>\n",
       "      <td>False</td>\n",
       "      <td>guess</td>\n",
       "      <td>34</td>\n",
       "      <td>1785</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label                   model  status   latency_s  parsed_json  \\\n",
       "1             LLaVA            llava:latest     200   89.551258        False   \n",
       "0  Llama-3.2-Vision  llama3.2-vision:latest     200  630.708090        False   \n",
       "2   Qwen2.5-VL (7B)        qwen2.5vl:latest     200  910.669698        False   \n",
       "\n",
       "  language  n_lines  n_chars  json_objs  degenerate  \n",
       "1    guess        1      246          0       False  \n",
       "0    guess       32     1887          0       False  \n",
       "2    guess       34     1785          0       False  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run comparison between 3 VLMs\n",
    "\n",
    "runs = []\n",
    "for m in MODELS:\n",
    "    try:\n",
    "        out = run_ollama_ocr(m[\"name\"], IMG, SYSTEM_PROMPT, BASE_OPTIONS)\n",
    "        parsed = parse_ocr_text(out[\"text\"])\n",
    "        runs.append({\n",
    "            \"label\": m[\"label\"],\n",
    "            \"model\": m[\"name\"],\n",
    "            \"status\": out[\"status\"],\n",
    "            \"latency_s\": out[\"latency_s\"],\n",
    "            \"parsed_json\": parsed[\"parsed_json\"],\n",
    "            \"language\": parsed[\"language\"],\n",
    "            \"n_lines\": len(parsed[\"lines\"]),\n",
    "            \"n_chars\": len(out[\"text\"] or \"\"),\n",
    "            \"json_objs\": parsed[\"json_objs\"],\n",
    "            \"degenerate\": looks_degenerate(out[\"text\"]),\n",
    "            \"lines\": parsed[\"lines\"],\n",
    "            \"text\": out[\"text\"],\n",
    "            \"preview\": \"\\n\".join(parsed[\"lines\"][:100]),\n",
    "        })\n",
    "        print(f\"OK  - {m['label']}  ({out['latency_s']:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        runs.append({\n",
    "            \"label\": m[\"label\"],\n",
    "            \"model\": m[\"name\"],\n",
    "            \"status\": None,\n",
    "            \"latency_s\": None,\n",
    "            \"parsed_json\": False,\n",
    "            \"language\": \"guess\",\n",
    "            \"n_lines\": 0,\n",
    "            \"n_chars\": 0,\n",
    "            \"json_objs\": 0,\n",
    "            \"degenerate\": True,\n",
    "            \"lines\": [],\n",
    "            \"text\": None,\n",
    "            \"preview\": \"\",\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "        print(f\"ERR - {m['label']}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(runs).sort_values(by=[\"degenerate\", \"parsed_json\", \"latency_s\"], ascending=[True, False, True])\n",
    "df[[\"label\",\"model\",\"status\",\"latency_s\",\"parsed_json\",\"language\",\"n_lines\",\"n_chars\",\"json_objs\",\"degenerate\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d40d84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba005632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### OCR Output — LLaVA \n",
       "- **Model:** llava:latest\n",
       "- **Latency:** 89.6s\n",
       "- **Parsed JSON:** False  |  **Degenerate:** False  |  **Language:** guess\n",
       "- **Showing:** 1/1 lines\n",
       "\n",
       "    ```text\n",
       "    {\"language\": \"eng\", \"lines\": [\"A traitor is someone who has been given something in return for their betrayal.\", \"Their loyalty to another person or group of people, which they have promised to protect and serve, is now compromised by this act.\"]\n",
       "    ```\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### OCR Output — Llama-3.2-Vision \n",
       "- **Model:** llama3.2-vision:latest\n",
       "- **Latency:** 630.7s\n",
       "- **Parsed JSON:** False  |  **Degenerate:** False  |  **Language:** guess\n",
       "- **Showing:** 32/32 lines\n",
       "\n",
       "    ```text\n",
       "    {\n",
       "    \"language\": \"por\",\n",
       "    \"lines\": [\n",
       "        \"A trilha sonora de um pais ideal\",\n",
       "        \"O\",\n",
       "        \"lha que coisa mais linda: as garotas de Ipanema-1961\",\n",
       "        \"tomavam cuba-libre, dirigiam Kharman-Glias e voavam\",\n",
       "        \"pela Panair. Usavam frasqueira, vestido-tubinho, cilio\",\n",
       "        \"postico, perua, laque. Diziam-se existencialistas, adoravam\",\n",
       "        \"arte abstrata e nao perdiam um filme da Nouvelle Vague.\",\n",
       "        \"Seus pontos eram o Beco das Garrafas, a Cinemateca, o Arpoador.\",\n",
       "        \"Iam a praia com a camisa social do irmao e, sob esta, um biquini que\",\n",
       "        \"de tao insolente, fazia o sangue dos rapazes ferver da maneira\",\n",
       "        \"mais incoveniente.\",\n",
       "        \"Tudo isso passou. A querida Panair nunca mais voou, a\",\n",
       "        \"Nouvelle Vague e um filme em preto e branco e ninguem mais\",\n",
       "        \"toma cuba-libre - quem pensaria hoje em misturar rum com\",\n",
       "        \"Coca-Cola? Quanto aquele biquini, era mesmo insolente, em-\",\n",
       "        \"bora, por padroes subsequentes, sua calinha contivesse pano\",\n",
       "        \"para fabricar dois ou tres para-ques. Dito assim, e como se, em\",\n",
       "        \"1961, o ceu do Brasil ainda fosse povoado por pterodactilos.\",\n",
       "        \"Mas ha uma excessao. A musica que aquelas garotas escutavam\",\n",
       "        \"na epoca continua a ser ouvida - um milenio depois - como se\",\n",
       "        \"brotasse das esferas: a Bossa Nova.\",\n",
       "        \"Acreditou ou nao, em numeros absolutos ouve-se mais\",\n",
       "        \"Bossa Nova hoje do que em 1961. Eela nao brota das esferas, mas\",\n",
       "        \"e produzida ao vivo, pelos gogos, dedos e pulmoes de artistas de\",\n",
       "        \"todas as idades, em lugares fechados ou ao ar livre, em quatro ou\",\n",
       "        \"cinco continentes. Ouve-se Bossa Nova em salas de concerto,\",\n",
       "        \"teatros, boates, bares, clubes, escolas, estadios, pracas, praiaas e\",\n",
       "        \"quiosques e, ultimamente, como uma epidemia, nas ruas notur-\"\n",
       "    ]\n",
       "    ```\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### OCR Output — Qwen2.5-VL (7B) \n",
       "- **Model:** qwen2.5vl:latest\n",
       "- **Latency:** 910.7s\n",
       "- **Parsed JSON:** False  |  **Degenerate:** False  |  **Language:** guess\n",
       "- **Showing:** 34/34 lines\n",
       "\n",
       "    ```text\n",
       "    {\n",
       "  \"language\": \"por\",\n",
       "  \"lines\": [\n",
       "    \"A trilha\",\n",
       "    \"sonora de um\",\n",
       "    \"país ideal\",\n",
       "    \"\",\n",
       "    \"Olha que coisa mais linda: as garotas de Ipanema-1961\",\n",
       "    \"tomavam cuba-libre, dirigiam Kharman-Ghias e voavam\",\n",
       "    \"pela Panair. Usavam frasqueira, vestido-tubinho, cílio\",\n",
       "    \"postiço, peruca, laquê. Diziam-se existencialistas, adoravam\",\n",
       "    \"arte abstrata e não perdiam um filme da Nouvelle Vague. Seus\",\n",
       "    \"points eram o Beco das Garrafas, a Cinemateca, o Arpoador. Iam\",\n",
       "    \"à praia com a camisa social do irmão e, sob esta, um biquíni que\",\n",
       "    \"de tão insolente, fazia o sangue dos rapazes ferver da maneira\",\n",
       "    \"mais inconveniente.\",\n",
       "    \"Tudo isso passou. A querida Panair nunca mais voou, a\",\n",
       "    \"Nouvelle Vague é um filme em preto e branco e ninguém mais\",\n",
       "    \"toma cuba-libre — quem pensaria hoje em misturar rum com\",\n",
       "    \"Coca-Cola? Quanto àquele biquíni, era mesmo insolente, embora\",\n",
       "    \"por padrões subsequentes, sua calcinha contivesse pano\",\n",
       "    \"para fabricar dois ou três pára-quedas. Dito assim, é como se, em\",\n",
       "    \"1961, o céu do Brasil ainda fosse povoado por pterodáctilos.\",\n",
       "    \"Mas há uma exceção. A música que aquelas garotas escutavam\",\n",
       "    \"na época continua a ser ouvida — um milênio depois —\",\n",
       "    \"como se brotasse das esferas: a Bossa Nova.\",\n",
       "    \"Acredite ou não, em números absolutos ouve-se mais Bossa\",\n",
       "    \"Nova hoje do que em 1961. E ela não brota das esferas, mas\",\n",
       "    \"é produzida ao vivo, pelos gogós, dedos e pulmões de artistas\",\n",
       "    \"de todas as idades, em lugares fechados ou ao ar livre, em quatro\",\n",
       "    \"ou cinco continentes. Ouviu-se Bossa Nova em salas de concerto\",\n",
       "    \"teatros, boates, bares, clubes, escolas, estádios, praças, praias\",\n",
       "    \"e quiosques e, ultimamente, como uma epidemia, nas ruas noturnas\"\n",
       "  ]\n",
       "    ```\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_SHOW = 200  # show more lines here\n",
    "\n",
    "for r in df.to_dict(orient=\"records\"):\n",
    "    label = r[\"label\"]\n",
    "    full_row = next(x for x in runs if x[\"label\"] == label)\n",
    "\n",
    "    lines = full_row.get(\"lines\", [])\n",
    "    content = \"\\n\".join(lines[:N_SHOW]) if lines else \"<empty>\"\n",
    "\n",
    "    latency = full_row.get(\"latency_s\")\n",
    "    latency_str = f\"{latency:.1f}s\" if isinstance(latency, (int, float)) else \"—\"\n",
    "\n",
    "    md = f\"\"\"### OCR Output — {label} \\n- **Model:** {full_row.get(\"model\")}\\n- **Latency:** {latency_str}\\n- **Parsed JSON:** {full_row.get(\"parsed_json\")}  |  **Degenerate:** {full_row.get(\"degenerate\")}  |  **Language:** {full_row.get(\"language\")}\\n- **Showing:** {min(N_SHOW, len(lines))}/{len(lines)} lines\n",
    "\n",
    "    ```text\n",
    "    {content}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    display(Markdown(md))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb839e4d",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "**Qualitative comparison (page-level):**\n",
    "- **LLaVA** failed to behave as an OCR transcriber in this setup and produced an unrelated English definition (hallucination). This indicates weak “verbatim transcription” compliance and/or unreliable document-text reading for dense page OCR.\n",
    "- **Llama-3.2-Vision** produced a coherent Portuguese transcription aligned with the page content. Main errors are typical OCR artifacts: missing diacritics (e.g., *cílios → cilio*), minor substitutions (*Kharman-Ghias → Kharman-Glias*), and occasional spacing/line-split artifacts (*O / lha*).\n",
    "- **Qwen2.5-VL** produced the most faithful transcription overall, preserving many diacritics and punctuation and maintaining strong semantic consistency with the page. Minor issues remain (e.g., occasional English leakage like “points”) and some differences in line wrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d34fe",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a1bbd",
   "metadata": {},
   "source": [
    "### Step G - Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c52065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MonReader_tools import join_hyphenated_linebreaks, normalize_for_eval, cer, wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76412c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground-truth for the first page\n",
    "gt_lines = [\n",
    "    \"A trilha\",\n",
    "    \"sonora de um\",\n",
    "    \"pais ideal\",\n",
    "    \"O\",\n",
    "    \"lha que coisa mais linda: as garotas de Ipanema-1961\",\n",
    "    \"tomavam cuba-libre, dirigiam Kharman-Ghias e voavam\",\n",
    "    \"pela Panair. Usavam frasqueira, vestido-tubinho, cílio\",\n",
    "    \"postiço, peruca, laquê. Diziam-se existencialistas, adoravam\",\n",
    "    \"arte abstrata e nao perdiam um filme da Nouvelle Vague. Seus\",\n",
    "    \"pointos eram o Beco das Garrafas, a Cinemateca, o Arpoador. Iam\",\n",
    "    \"à praia com a camisa social do irmão e, sob esta, um biquini que,\",\n",
    "    \"de tão insolente, fazia o sangue dos rapazes ferver da maneira\",\n",
    "    \"mais incoveniente.\",\n",
    "    \"Tudo isso passou. A querida Panair nunca mais voou, a\",\n",
    "    \"Nouvelle Vague e um filme em preto e branco e ninguém mais\",\n",
    "    \"toma cuba-libre - quem pensaria hoje em misturar rum com\",\n",
    "    \"Coca-Cola? Quanto àquele biquíni, era mesmo insolente, em-\",\n",
    "    \"bora, por padrões subseqüentes, sua calinha contivesse pano\",\n",
    "    \"para fabricar dois ou tres pára-quedas. Dito assim, é como se, em\",\n",
    "    \"1961, o céu do Brasil ainda fosse povoado por pterodáctilos.\",\n",
    "    \"Mas há uma excessao. A música que aquelas garotas escu-\",\n",
    "    \"tavam na época continua a ser ouvida - um milênio depois -\",\n",
    "    \"como se brotasse das esferas: a Bossa Nova.\",\n",
    "    \"Acredite ou não, em números absolutos ouve-se mais Bos-\",\n",
    "    \"sa Nova hoje do que em 1961. E ela não brota das esferas, mas\",\n",
    "    \"é produzida ao vivo, pelos gogós, dedos e pulmões de artistas de\",\n",
    "    \"todas as idades, em lugares fechados ou ao ar livre, em quatro ou\",\n",
    "    \"cinco continentes. Ouve-se Bossa Nova em salas de concerto,\",\n",
    "    \"teatros, boates, bares, clubes, escolas, estádios, praças, praiaas e\",\n",
    "    \"quiosques e, ultimamente, como uma epidemia, nas ruas notur-\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ebf8256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: 25\n",
      "A trilha\n",
      "sonora de um\n",
      "pais ideal\n",
      "ha que coisa mais rica: às Saroias de Ipanerna-1961 lomavam cuba-libie, dirigiam Kbarman-Ghuias e voavam Dela Panair Usava Írasqueira, vesudo-tubinho, ciho\n",
      "POSTIÇO, PETUÇA, sague Diziam se existencialistas, adoravarm\n",
      "Urte apstiata e não perdiiin um filme da Nouvelle Vague. Seus\n",
      "DEOSRIS ETA O Beco tas Garrafas, a CQinernateca, 0 At poador. lam\n",
      "SC PTE COM A CAMISA social do irmao v sob esta, um biuguim que,\n",
      "DES MNA de tão insortente, fazia 0 sangue dos rapazes ferver da maneira\n",
      "TAIS INCONYCnicNto.\n",
      "Tudo 1sso passou. À quenda Panair nunca mais VOOU, à\n",
      "Nouvelle Vague ce um filme em preto e branco e Musuem mais\n",
      "toma cuba-ltbre -\n",
      "7 QUEIN pensaria hoje em misturar rum com\n",
      "Coca-Cola? Quanto aquele biquini, era mesmo mmsolente, embora, por padrões subsequentes, sua caleinha CONtIvVesse pano\n",
      "para Gtbricar dois vu três pára quedas. Dito assim, c como se, em\n",
      "1901, 0 ceu do Brasil anda fosse povoado por pterodactilos,\n",
      "Mas ha uma exceção. À música que aquelas garotas escuTAVAM Na epoca continua à ser ouvida -\n",
      "- um miténio depois como se brotasse das esferas: a Bossa Nova\n",
      "Acredite ou não, em números absolutos OUVe-se mais BosA Inova hoje do que em 1961. E ela não brota das esferas, mas\n",
      "€ produzida ao vivo, pelos gogos, dedos e pulmões de artistas de\n",
      "todas as idades, em lugares fechados ou ao ar livre, em quarro ou\n",
      "FUNCO Continentes, Quve-se Bossa Nova em salas de concerto,\n",
      "téatros, boates, bares, clubes, escolas, estádios, praças, praias e\n",
      "Quiosques e, ultimamente, como uma epidemia, nas ruas notur-\n"
     ]
    }
   ],
   "source": [
    "# import the Tesseract extraction file corresponding to page12 (from MonReader_part2)\n",
    "tess_path = Path(r\"E:\\Devs\\pyEnv-1\\Apziva\\MonReader\\data\\books\\A_onda_que_se_ergueu_no_mar-Ruy_Castro\\tesseract_extraction_pag12.txt\")\n",
    "\n",
    "with tess_path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    tesseract_lines = [line.rstrip(\"\\n\") for line in f] \n",
    "\n",
    "print(f\"Lines: {len(tesseract_lines)}\")\n",
    "print(\"\\n\".join(tesseract_lines[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2334ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseract_run = {\n",
    "    \"label\": \"Tesseract\",\n",
    "    \"model\": \"tesseract-ocr\",\n",
    "    \"status\": 200,\n",
    "    \"latency_s\": None,\n",
    "    \"parsed_json\": True,\n",
    "    \"language\": \"por\",\n",
    "    \"n_lines\": len(tesseract_lines),\n",
    "    \"n_chars\": sum(len(l) for l in tesseract_lines),\n",
    "    \"json_objs\": 1,\n",
    "    \"degenerate\": False,\n",
    "    \"lines\": tesseract_lines,\n",
    "    \"text\": \"\\n\".join(tesseract_lines),\n",
    "    \"preview\": \"\\n\".join(tesseract_lines[:8]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15cb4b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Llama-3.2-Vision',\n",
       "  'CER': 0.11979166666666667,\n",
       "  'WER': 0.34765625,\n",
       "  'ref_chars': 1536,\n",
       "  'hyp_chars': 1681},\n",
       " {'label': 'LLaVA',\n",
       "  'CER': 0.8834635416666666,\n",
       "  'WER': 1.0,\n",
       "  'ref_chars': 1536,\n",
       "  'hyp_chars': 246},\n",
       " {'label': 'Qwen2.5-VL (7B)',\n",
       "  'CER': 0.11393229166666667,\n",
       "  'WER': 0.29296875,\n",
       "  'ref_chars': 1536,\n",
       "  'hyp_chars': 1691},\n",
       " {'label': 'Tesseract',\n",
       "  'CER': 0.13346354166666666,\n",
       "  'WER': 0.3828125,\n",
       "  'ref_chars': 1536,\n",
       "  'hyp_chars': 1539}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate every model in `runs`\n",
    "results = []\n",
    "runs.append(tesseract_run)\n",
    "\n",
    "for r in runs:\n",
    "    pred_lines = r.get(\"lines\", [])\n",
    "    ref = normalize_for_eval(gt_lines, keep_punct=True, keep_accents=True)\n",
    "    hyp = normalize_for_eval(pred_lines, keep_punct=True, keep_accents=True)\n",
    "\n",
    "    results.append({\n",
    "        \"label\": r[\"label\"],\n",
    "        \"CER\": cer(ref, hyp),\n",
    "        \"WER\": wer(ref, hyp),\n",
    "        \"ref_chars\": len(ref),\n",
    "        \"hyp_chars\": len(hyp),\n",
    "    })\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d71033",
   "metadata": {},
   "source": [
    "### Quantitative evaluation (against ground truth + Tesseract)\n",
    "\n",
    "**Metrics**\n",
    "- **CER (Character Error Rate):** the minimum number of character edits (insertions + deletions + substitutions) required to transform the prediction into the ground truth, divided by the number of characters in the ground truth. Lower is better.\n",
    "- **WER (Word Error Rate):** the minimum number of word edits (insertions + deletions + substitutions) required to transform the prediction into the ground truth, divided by the number of words in the ground truth. Lower is better.\n",
    "\n",
    "**Results (lower is better):**\n",
    "- **Qwen2.5-VL:** CER ≈ **0.114**, WER ≈ **0.293**  *(best overall)*\n",
    "- **Llama-3.2-Vision:** CER ≈ **0.120**, WER ≈ **0.348**\n",
    "- **Tesseract:** CER ≈ **0.133**, WER ≈ **0.383**\n",
    "- **LLaVA:** CER ≈ **0.883**, WER = **1.0** *(not usable as OCR here)*\n",
    "\n",
    "**Takeaway**\n",
    "- For this page, **Qwen2.5-VL** achieved the best transcription quality, followed by **Llama-3.2-Vision**.  \n",
    "- Both VLMs outperform **Tesseract** under the same raw-image conditions (no pre-processing).  \n",
    "- **LLaVA** is excluded from further OCR comparisons unless prompt/model settings are changed to reliably enforce transcription behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d15776",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bccaa5",
   "metadata": {},
   "source": [
    "### Step H — Full Extraction (Qwen2.5-VL) on Both Books (No Pre-processing)\n",
    "\n",
    "**Goal.**  \n",
    "Run **Qwen2.5-VL** (GGUF via Ollama) on *every* page image from both books, using the same verbatim JSON OCR prompt and decoding options that performed best in Step G.\n",
    "\n",
    "**Outputs.**\n",
    "- One `.json` per page (raw model output)\n",
    "- One `.txt` per page (human-readable transcription)\n",
    "- One `manifest.csv` summarizing latency, parse success, number of lines/chars, and error notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f77c8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step H config\n",
    "\n",
    "QWEN_MODEL = \"qwen2.5vl:latest\"\n",
    "STEPH_DIR = WORK_DIR / \"stepH_qwen2p5vl_full\"\n",
    "STEPH_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22f40116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layout\n",
    "\n",
    "# work/stepH_qwen2p5vl_full/\n",
    "#   The_Chamber-John_Grisham/\n",
    "#     json/\n",
    "#     txt/\n",
    "#     manifest.csv\n",
    "#   A_onda_que_se_ergueu_no_mar-Ruy_Castro/\n",
    "#     json/\n",
    "#     txt/\n",
    "#     manifest.csv\n",
    "\n",
    "BOOKS = [\n",
    "    {\"book_id\": \"The_Chamber-John_Grisham\", \"img_dir\": ENG_IMG_DIR, \"lang_hint\": \"eng\"},\n",
    "    {\"book_id\": \"A_onda_que_se_ergueu_no_mar-Ruy_Castro\", \"img_dir\": POR_IMG_DIR, \"lang_hint\": \"por\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e177393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import MonReader_tools\n",
    "\n",
    "importlib.reload(MonReader_tools)\n",
    "\n",
    "from MonReader_tools import (\n",
    "    list_images_sorted, safe_write_text, safe_write_json,\n",
    "    run_qwen_on_image, already_done\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438eb461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Step H: The_Chamber-John_Grisham <<<\n",
      "Found 12 images in: e:\\Devs\\pyEnv-1\\Apziva\\MonReader\\data\\books\\The_Chamber-John_Grisham\\images\n",
      "[   2/12] OCR: pag2.JPEG\n",
      "[   3/12] OCR: pag4.JPEG\n",
      "[   4/12] OCR: pag6.JPEG\n",
      "[   5/12] OCR: pag8.JPEG\n",
      "[   6/12] OCR: pag10.JPEG\n",
      "[   7/12] OCR: pag12.JPEG\n",
      "[   8/12] OCR: pag14.JPEG\n",
      "[   9/12] OCR: pag16.JPEG\n",
      "[  10/12] OCR: pag18.JPEG\n",
      "[  11/12] OCR: pag20.JPEG\n",
      "[  12/12] OCR: pag22.JPEG\n",
      "Saved manifest: e:\\Devs\\pyEnv-1\\Apziva\\MonReader\\work\\stepH_qwen2p5vl_full\\The_Chamber-John_Grisham\\manifest.csv\n",
      "        image  status    latency_s  parsed_json language  n_lines degenerate  \\\n",
      "0   pag0.JPEG     200          NaN         True  unknown      NaN       None   \n",
      "1   pag2.JPEG     200  1003.389723        False    guess     68.0      False   \n",
      "2   pag4.JPEG     200   930.246915        False    guess     66.0      False   \n",
      "3   pag6.JPEG     200   943.951212        False    guess     62.0      False   \n",
      "4   pag8.JPEG     200   947.280215        False    guess     63.0      False   \n",
      "5  pag10.JPEG     200   962.241473        False    guess     69.0      False   \n",
      "6  pag12.JPEG     200   989.154797        False    guess     64.0      False   \n",
      "7  pag14.JPEG     200   969.046452        False    guess     66.0      False   \n",
      "8  pag16.JPEG     200   939.067333        False    guess     65.0      False   \n",
      "9  pag18.JPEG     200   937.715586        False    guess     67.0      False   \n",
      "\n",
      "              error  \n",
      "0  skipped_existing  \n",
      "1              None  \n",
      "2              None  \n",
      "3              None  \n",
      "4              None  \n",
      "5              None  \n",
      "6              None  \n",
      "7              None  \n",
      "8              None  \n",
      "9              None  \n",
      "\n",
      ">>> Step H: A_onda_que_se_ergueu_no_mar-Ruy_Castro <<<\n",
      "Found 15 images in: e:\\Devs\\pyEnv-1\\Apziva\\MonReader\\data\\books\\A_onda_que_se_ergueu_no_mar-Ruy_Castro\\images\n",
      "[   1/15] OCR: pag12.JPEG\n",
      "[   2/15] OCR: pag14.JPEG\n",
      "   -> ERROR: Ollama error: an error was encountered while running the model: GGML_ASSERT(a->ne[2] * 4 == b->ne[0]) failed\n",
      "[   3/15] OCR: pag16.JPEG\n",
      "   -> ERROR: Ollama error: an error was encountered while running the model: GGML_ASSERT(a->ne[2] * 4 == b->ne[0]) failed\n",
      "[   4/15] OCR: pag18.JPEG\n",
      "[   5/15] OCR: pag20.JPEG\n"
     ]
    }
   ],
   "source": [
    "# Batch run (both books)\n",
    "\n",
    "all_manifests = {}\n",
    "\n",
    "for b in BOOKS:\n",
    "    book_id = b[\"book_id\"]\n",
    "    img_dir = b[\"img_dir\"]\n",
    "    book_out = STEPH_DIR / book_id\n",
    "    json_dir = book_out / \"json\"\n",
    "    txt_dir  = book_out / \"txt\"\n",
    "    book_out.mkdir(parents=True, exist_ok=True)\n",
    "    json_dir.mkdir(parents=True, exist_ok=True)\n",
    "    txt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    imgs = list_images_sorted(img_dir)\n",
    "    print(f\"\\n>>> Step H: {book_id} <<<\")\n",
    "    print(f\"Found {len(imgs)} images in: {img_dir}\")\n",
    "\n",
    "    manifest_rows = []\n",
    "    for i, img_path in enumerate(imgs, 1):\n",
    "        stem = img_path.stem  \n",
    "        out_json_path = json_dir / f\"{stem}.json\"\n",
    "        out_txt_path  = txt_dir  / f\"{stem}.txt\"\n",
    "\n",
    "        # Resume behavior: skip pages that already have outputs\n",
    "        if already_done(out_txt_path, out_json_path):\n",
    "            manifest_rows.append({\n",
    "                \"image\": img_path.name,\n",
    "                \"image_path\": str(img_path),\n",
    "                \"model\": QWEN_MODEL,\n",
    "                \"status\": 200,\n",
    "                \"latency_s\": None,\n",
    "                \"parsed_json\": True,\n",
    "                \"language\": \"unknown\",\n",
    "                \"json_objs\": None,\n",
    "                \"n_lines\": None,\n",
    "                \"n_chars_raw\": None,\n",
    "                \"n_chars_txt\": None,\n",
    "                \"degenerate\": None,\n",
    "                \"parse_error\": None,\n",
    "                \"error\": \"skipped_existing\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        print(f\"[{i:>4}/{len(imgs)}] OCR: {img_path.name}\")\n",
    "        try:\n",
    "            row, raw_text, lines = run_qwen_on_image(\n",
    "                img_path,\n",
    "                model_name=QWEN_MODEL,\n",
    "                prompt=SYSTEM_PROMPT,\n",
    "                options=BASE_OPTIONS,\n",
    "                run_ollama_ocr_fn=run_ollama_ocr,\n",
    "                parse_ocr_text_fn=parse_ocr_text,\n",
    "                looks_degenerate_fn=looks_degenerate,\n",
    "            )\n",
    "\n",
    "            # Save raw response text as JSON wrapper (debuggable) + parsed transcription\n",
    "            # 1) JSON file: keep raw model response + parsed fields for reproducibility\n",
    "            save_obj = {\n",
    "                \"image\": img_path.name,\n",
    "                \"image_path\": str(img_path),\n",
    "                \"model\": QWEN_MODEL,\n",
    "                \"prompt\": SYSTEM_PROMPT,\n",
    "                \"options\": BASE_OPTIONS,\n",
    "                \"timestamp_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "                \"raw_response\": raw_text,\n",
    "                \"parsed\": {\n",
    "                    \"language\": row[\"language\"],\n",
    "                    \"parsed_json\": row[\"parsed_json\"],\n",
    "                    \"json_objs\": row[\"json_objs\"],\n",
    "                    \"lines\": lines,\n",
    "                    \"parse_error\": row[\"parse_error\"],\n",
    "                    \"degenerate\": row[\"degenerate\"],\n",
    "                },\n",
    "                \"latency_s\": row[\"latency_s\"],\n",
    "                \"status\": row[\"status\"],\n",
    "            }\n",
    "            safe_write_json(out_json_path, save_obj)\n",
    "\n",
    "            # 2) TXT file: just the transcription lines\n",
    "            safe_write_text(out_txt_path, \"\\n\".join(lines))\n",
    "\n",
    "            manifest_rows.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            manifest_rows.append({\n",
    "                \"image\": img_path.name,\n",
    "                \"image_path\": str(img_path),\n",
    "                \"model\": QWEN_MODEL,\n",
    "                \"status\": None,\n",
    "                \"latency_s\": None,\n",
    "                \"parsed_json\": False,\n",
    "                \"language\": \"guess\",\n",
    "                \"json_objs\": 0,\n",
    "                \"n_lines\": 0,\n",
    "                \"n_chars_raw\": 0,\n",
    "                \"n_chars_txt\": 0,\n",
    "                \"degenerate\": True,\n",
    "                \"parse_error\": None,\n",
    "                \"error\": repr(e),\n",
    "            })\n",
    "            print(f\"   -> ERROR: {e}\")\n",
    "\n",
    "    df_book = pd.DataFrame(manifest_rows)\n",
    "    manifest_path = book_out / \"manifest.csv\"\n",
    "    df_book.to_csv(manifest_path, index=False, encoding=\"utf-8\")\n",
    "    all_manifests[book_id] = df_book\n",
    "\n",
    "    print(f\"Saved manifest: {manifest_path}\")\n",
    "    print(df_book[[\"image\",\"status\",\"latency_s\",\"parsed_json\",\"language\",\"n_lines\",\"degenerate\",\"error\"]].head(10))\n",
    "\n",
    "# combined manifest across both books\n",
    "df_all = pd.concat(\n",
    "    [all_manifests[k].assign(book=k) for k in all_manifests.keys()],\n",
    "    ignore_index=True\n",
    ")\n",
    "combined_manifest_path = STEPH_DIR / \"manifest_all_books.csv\"\n",
    "df_all.to_csv(combined_manifest_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved combined manifest: {combined_manifest_path}\")\n",
    "\n",
    "df_all[[\"book\",\"image\",\"status\",\"latency_s\",\"parsed_json\",\"language\",\"n_lines\",\"degenerate\",\"error\"]].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840af0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MonReader_env_part3",
   "language": "python",
   "name": "monreader_env_part3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
